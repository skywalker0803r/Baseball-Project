{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d16797d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FF\n",
      "features\\Yu_Darvish_FF_videos_4S\\train ÂÖ±Êúâ 156 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FS\n",
      "features\\Yu_Darvish_FS_videos_4S\\train ÂÖ±Êúâ 149 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_SL\n",
      "features\\Yu_Darvish_SL_videos_4S\\train ÂÖ±Êúâ 153 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FF\n",
      "features\\Yu_Darvish_FF_videos_4S\\test ÂÖ±Êúâ 40 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FS\n",
      "features\\Yu_Darvish_FS_videos_4S\\test ÂÖ±Êúâ 38 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_SL\n",
      "features\\Yu_Darvish_SL_videos_4S\\test ÂÖ±Êúâ 40 ÂÄã .npy Ê™îÊ°à\n",
      "(458, 4000)\n",
      "üìä Ë®ìÁ∑¥Ë≥áÊñôÁ≠ÜÊï∏Ôºö458ÔºåÊ∏¨Ë©¶Ë≥áÊñôÁ≠ÜÊï∏Ôºö118\n",
      "Epoch 1/200, Loss: 4.5547\n",
      "Epoch 2/200, Loss: 87.1511\n",
      "Epoch 3/200, Loss: 6.2488\n",
      "Epoch 4/200, Loss: 23.3607\n",
      "Epoch 5/200, Loss: 26.3339\n",
      "Epoch 6/200, Loss: 20.3076\n",
      "Epoch 7/200, Loss: 11.6812\n",
      "Epoch 8/200, Loss: 4.6812\n",
      "Epoch 9/200, Loss: 9.4542\n",
      "Epoch 10/200, Loss: 10.0811\n",
      "Epoch 11/200, Loss: 4.9736\n",
      "Epoch 12/200, Loss: 3.9309\n",
      "Epoch 13/200, Loss: 5.4635\n",
      "Epoch 14/200, Loss: 6.2284\n",
      "Epoch 15/200, Loss: 5.8994\n",
      "Epoch 16/200, Loss: 4.4993\n",
      "Epoch 17/200, Loss: 3.0845\n",
      "Epoch 18/200, Loss: 3.4399\n",
      "Epoch 19/200, Loss: 4.5283\n",
      "Epoch 20/200, Loss: 4.1863\n",
      "Epoch 21/200, Loss: 2.9432\n",
      "Epoch 22/200, Loss: 2.0489\n",
      "Epoch 23/200, Loss: 2.6473\n",
      "Epoch 24/200, Loss: 2.6969\n",
      "Epoch 25/200, Loss: 2.9194\n",
      "Epoch 26/200, Loss: 2.4555\n",
      "Epoch 27/200, Loss: 2.2681\n",
      "Epoch 28/200, Loss: 1.8644\n",
      "Epoch 29/200, Loss: 1.9166\n",
      "Epoch 30/200, Loss: 2.2381\n",
      "Epoch 31/200, Loss: 2.2176\n",
      "Epoch 32/200, Loss: 1.6822\n",
      "Epoch 33/200, Loss: 1.7618\n",
      "Epoch 34/200, Loss: 1.7776\n",
      "Epoch 35/200, Loss: 1.4986\n",
      "Epoch 36/200, Loss: 1.5820\n",
      "Epoch 37/200, Loss: 1.6263\n",
      "Epoch 38/200, Loss: 1.2081\n",
      "Epoch 39/200, Loss: 1.2441\n",
      "Epoch 40/200, Loss: 1.2556\n",
      "Epoch 41/200, Loss: 1.3700\n",
      "Epoch 42/200, Loss: 1.1963\n",
      "Epoch 43/200, Loss: 1.1522\n",
      "Epoch 44/200, Loss: 1.0281\n",
      "Epoch 45/200, Loss: 1.1092\n",
      "Epoch 46/200, Loss: 1.0723\n",
      "Epoch 47/200, Loss: 1.0963\n",
      "Epoch 48/200, Loss: 0.8837\n",
      "Epoch 49/200, Loss: 0.9416\n",
      "Epoch 50/200, Loss: 0.9880\n",
      "Epoch 51/200, Loss: 0.9866\n",
      "Epoch 52/200, Loss: 0.9646\n",
      "Epoch 53/200, Loss: 0.8129\n",
      "Epoch 54/200, Loss: 0.7463\n",
      "Epoch 55/200, Loss: 0.8047\n",
      "Epoch 56/200, Loss: 0.8358\n",
      "Epoch 57/200, Loss: 0.7946\n",
      "Epoch 58/200, Loss: 0.7630\n",
      "Epoch 59/200, Loss: 0.7824\n",
      "Epoch 60/200, Loss: 0.7651\n",
      "Epoch 61/200, Loss: 0.7532\n",
      "Epoch 62/200, Loss: 0.7538\n",
      "Epoch 63/200, Loss: 0.7686\n",
      "Epoch 64/200, Loss: 0.7308\n",
      "Epoch 65/200, Loss: 0.7315\n",
      "Epoch 66/200, Loss: 0.6899\n",
      "Epoch 67/200, Loss: 0.6875\n",
      "Epoch 68/200, Loss: 0.6774\n",
      "Epoch 69/200, Loss: 0.6787\n",
      "Epoch 70/200, Loss: 0.6990\n",
      "Epoch 71/200, Loss: 0.6670\n",
      "Epoch 72/200, Loss: 0.6984\n",
      "Epoch 73/200, Loss: 0.6532\n",
      "Epoch 74/200, Loss: 0.6908\n",
      "Epoch 75/200, Loss: 0.6744\n",
      "Epoch 76/200, Loss: 0.6326\n",
      "Epoch 77/200, Loss: 0.6974\n",
      "Epoch 78/200, Loss: 0.6614\n",
      "Epoch 79/200, Loss: 0.6430\n",
      "Epoch 80/200, Loss: 0.6303\n",
      "Epoch 81/200, Loss: 0.6594\n",
      "Epoch 82/200, Loss: 0.6326\n",
      "Epoch 83/200, Loss: 0.6412\n",
      "Epoch 84/200, Loss: 0.6210\n",
      "Epoch 85/200, Loss: 0.6555\n",
      "Epoch 86/200, Loss: 0.6142\n",
      "Epoch 87/200, Loss: 0.6091\n",
      "Epoch 88/200, Loss: 0.6305\n",
      "Epoch 89/200, Loss: 0.6299\n",
      "Epoch 90/200, Loss: 0.6183\n",
      "Epoch 91/200, Loss: 0.6254\n",
      "Epoch 92/200, Loss: 0.6247\n",
      "Epoch 93/200, Loss: 0.6210\n",
      "Epoch 94/200, Loss: 0.6282\n",
      "Epoch 95/200, Loss: 0.6049\n",
      "Epoch 96/200, Loss: 0.6216\n",
      "Epoch 97/200, Loss: 0.6179\n",
      "Epoch 98/200, Loss: 0.5887\n",
      "Epoch 99/200, Loss: 0.6224\n",
      "Epoch 100/200, Loss: 0.5989\n",
      "Epoch 101/200, Loss: 0.6119\n",
      "Epoch 102/200, Loss: 0.6179\n",
      "Epoch 103/200, Loss: 0.6190\n",
      "Epoch 104/200, Loss: 0.6169\n",
      "Epoch 105/200, Loss: 0.6084\n",
      "Epoch 106/200, Loss: 0.6275\n",
      "Epoch 107/200, Loss: 0.6031\n",
      "Epoch 108/200, Loss: 0.5970\n",
      "Epoch 109/200, Loss: 0.6034\n",
      "Epoch 110/200, Loss: 0.6027\n",
      "Epoch 111/200, Loss: 0.5958\n",
      "Epoch 112/200, Loss: 0.5976\n",
      "Epoch 113/200, Loss: 0.6149\n",
      "Epoch 114/200, Loss: 0.5990\n",
      "Epoch 115/200, Loss: 0.6083\n",
      "Epoch 116/200, Loss: 0.6142\n",
      "Epoch 117/200, Loss: 0.6177\n",
      "Epoch 118/200, Loss: 0.5968\n",
      "Epoch 119/200, Loss: 0.6194\n",
      "Epoch 120/200, Loss: 0.6001\n",
      "Epoch 121/200, Loss: 0.6200\n",
      "Epoch 122/200, Loss: 0.6060\n",
      "Epoch 123/200, Loss: 0.6091\n",
      "Epoch 124/200, Loss: 0.6003\n",
      "Epoch 125/200, Loss: 0.5934\n",
      "Epoch 126/200, Loss: 0.5984\n",
      "Epoch 127/200, Loss: 0.5912\n",
      "Epoch 128/200, Loss: 0.6062\n",
      "Epoch 129/200, Loss: 0.5921\n",
      "Epoch 130/200, Loss: 0.6041\n",
      "Epoch 131/200, Loss: 0.6060\n",
      "Epoch 132/200, Loss: 0.6072\n",
      "Epoch 133/200, Loss: 0.6044\n",
      "Epoch 134/200, Loss: 0.6005\n",
      "Epoch 135/200, Loss: 0.5996\n",
      "Epoch 136/200, Loss: 0.5853\n",
      "Epoch 137/200, Loss: 0.5902\n",
      "Epoch 138/200, Loss: 0.6011\n",
      "Epoch 139/200, Loss: 0.6068\n",
      "Epoch 140/200, Loss: 0.6016\n",
      "Epoch 141/200, Loss: 0.5901\n",
      "Epoch 142/200, Loss: 0.5942\n",
      "Epoch 143/200, Loss: 0.6025\n",
      "Epoch 144/200, Loss: 0.6067\n",
      "Epoch 145/200, Loss: 0.6015\n",
      "Epoch 146/200, Loss: 0.6097\n",
      "Epoch 147/200, Loss: 0.6019\n",
      "Epoch 148/200, Loss: 0.5787\n",
      "Epoch 149/200, Loss: 0.5946\n",
      "Epoch 150/200, Loss: 0.6046\n",
      "Epoch 151/200, Loss: 0.6009\n",
      "Epoch 152/200, Loss: 0.5868\n",
      "Epoch 153/200, Loss: 0.5944\n",
      "Epoch 154/200, Loss: 0.5874\n",
      "Epoch 155/200, Loss: 0.5902\n",
      "Epoch 156/200, Loss: 0.5912\n",
      "Epoch 157/200, Loss: 0.6082\n",
      "Epoch 158/200, Loss: 0.5938\n",
      "Epoch 159/200, Loss: 0.5909\n",
      "Epoch 160/200, Loss: 0.5843\n",
      "Epoch 161/200, Loss: 0.6035\n",
      "Epoch 162/200, Loss: 0.5935\n",
      "Epoch 163/200, Loss: 0.5964\n",
      "Epoch 164/200, Loss: 0.5923\n",
      "Epoch 165/200, Loss: 0.5885\n",
      "Epoch 166/200, Loss: 0.5874\n",
      "Epoch 167/200, Loss: 0.5872\n",
      "Epoch 168/200, Loss: 0.5851\n",
      "Epoch 169/200, Loss: 0.5919\n",
      "Epoch 170/200, Loss: 0.5871\n",
      "Epoch 171/200, Loss: 0.5808\n",
      "Epoch 172/200, Loss: 0.5933\n",
      "Epoch 173/200, Loss: 0.5876\n",
      "Epoch 174/200, Loss: 0.5837\n",
      "Epoch 175/200, Loss: 0.5919\n",
      "Epoch 176/200, Loss: 0.5956\n",
      "Epoch 177/200, Loss: 0.5809\n",
      "Epoch 178/200, Loss: 0.5844\n",
      "Epoch 179/200, Loss: 0.5832\n",
      "Epoch 180/200, Loss: 0.5755\n",
      "Epoch 181/200, Loss: 0.5909\n",
      "Epoch 182/200, Loss: 0.5976\n",
      "Epoch 183/200, Loss: 0.5821\n",
      "Epoch 184/200, Loss: 0.5800\n",
      "Epoch 185/200, Loss: 0.5854\n",
      "Epoch 186/200, Loss: 0.5803\n",
      "Epoch 187/200, Loss: 0.5734\n",
      "Epoch 188/200, Loss: 0.5728\n",
      "Epoch 189/200, Loss: 0.5900\n",
      "Epoch 190/200, Loss: 0.5978\n",
      "Epoch 191/200, Loss: 0.5774\n",
      "Epoch 192/200, Loss: 0.5689\n",
      "Epoch 193/200, Loss: 0.5760\n",
      "Epoch 194/200, Loss: 0.5886\n",
      "Epoch 195/200, Loss: 0.5724\n",
      "Epoch 196/200, Loss: 0.5691\n",
      "Epoch 197/200, Loss: 0.5742\n",
      "Epoch 198/200, Loss: 0.5748\n",
      "Epoch 199/200, Loss: 0.5696\n",
      "Epoch 200/200, Loss: 0.5763\n",
      "‚úÖ Ë®ìÁ∑¥Ê≠£Á¢∫ÁéáÔºö70.74%\n",
      "‚úÖ Ê∏¨Ë©¶Ê≠£Á¢∫ÁéáÔºö75.42%\n",
      "üíæ Ê®°ÂûãÂ∑≤ÂÑ≤Â≠òÁÇ∫ model_strike_predictor_debug.pth\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FF\n",
      "features\\Yu_Darvish_FF_videos_4S\\train ÂÖ±Êúâ 156 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FS\n",
      "features\\Yu_Darvish_FS_videos_4S\\train ÂÖ±Êúâ 149 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_SL\n",
      "features\\Yu_Darvish_SL_videos_4S\\train ÂÖ±Êúâ 153 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FF\n",
      "features\\Yu_Darvish_FF_videos_4S\\test ÂÖ±Êúâ 40 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FS\n",
      "features\\Yu_Darvish_FS_videos_4S\\test ÂÖ±Êúâ 38 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_SL\n",
      "features\\Yu_Darvish_SL_videos_4S\\test ÂÖ±Êúâ 40 ÂÄã .npy Ê™îÊ°à\n",
      "(458, 4000)\n",
      "üìä Ë®ìÁ∑¥Ë≥áÊñôÁ≠ÜÊï∏Ôºö458ÔºåÊ∏¨Ë©¶Ë≥áÊñôÁ≠ÜÊï∏Ôºö118\n",
      "Epoch 1/200, Loss: 4.5275\n",
      "Epoch 2/200, Loss: 39.4593\n",
      "Epoch 3/200, Loss: 25.9275\n",
      "Epoch 4/200, Loss: 13.5635\n",
      "Epoch 5/200, Loss: 3.6319\n",
      "Epoch 6/200, Loss: 19.0042\n",
      "Epoch 7/200, Loss: 12.2834\n",
      "Epoch 8/200, Loss: 3.7200\n",
      "Epoch 9/200, Loss: 7.2384\n",
      "Epoch 10/200, Loss: 8.5300\n",
      "Epoch 11/200, Loss: 7.9334\n",
      "Epoch 12/200, Loss: 6.0491\n",
      "Epoch 13/200, Loss: 3.1197\n",
      "Epoch 14/200, Loss: 3.4793\n",
      "Epoch 15/200, Loss: 5.2399\n",
      "Epoch 16/200, Loss: 4.8352\n",
      "Epoch 17/200, Loss: 3.2748\n",
      "Epoch 18/200, Loss: 2.7143\n",
      "Epoch 19/200, Loss: 3.6026\n",
      "Epoch 20/200, Loss: 3.8950\n",
      "Epoch 21/200, Loss: 3.0630\n",
      "Epoch 22/200, Loss: 2.3530\n",
      "Epoch 23/200, Loss: 2.1322\n",
      "Epoch 24/200, Loss: 2.7279\n",
      "Epoch 25/200, Loss: 2.2696\n",
      "Epoch 26/200, Loss: 1.8540\n",
      "Epoch 27/200, Loss: 1.9603\n",
      "Epoch 28/200, Loss: 1.8804\n",
      "Epoch 29/200, Loss: 2.0955\n",
      "Epoch 30/200, Loss: 1.5891\n",
      "Epoch 31/200, Loss: 1.4817\n",
      "Epoch 32/200, Loss: 1.5226\n",
      "Epoch 33/200, Loss: 1.4337\n",
      "Epoch 34/200, Loss: 1.4244\n",
      "Epoch 35/200, Loss: 1.2272\n",
      "Epoch 36/200, Loss: 1.2024\n",
      "Epoch 37/200, Loss: 1.3241\n",
      "Epoch 38/200, Loss: 1.1749\n",
      "Epoch 39/200, Loss: 1.0287\n",
      "Epoch 40/200, Loss: 1.1042\n",
      "Epoch 41/200, Loss: 1.0746\n",
      "Epoch 42/200, Loss: 1.0949\n",
      "Epoch 43/200, Loss: 0.9580\n",
      "Epoch 44/200, Loss: 0.9155\n",
      "Epoch 45/200, Loss: 0.9258\n",
      "Epoch 46/200, Loss: 0.9433\n",
      "Epoch 47/200, Loss: 0.8961\n",
      "Epoch 48/200, Loss: 0.8678\n",
      "Epoch 49/200, Loss: 0.8619\n",
      "Epoch 50/200, Loss: 0.8130\n",
      "Epoch 51/200, Loss: 0.7702\n",
      "Epoch 52/200, Loss: 0.8223\n",
      "Epoch 53/200, Loss: 0.7827\n",
      "Epoch 54/200, Loss: 0.7690\n",
      "Epoch 55/200, Loss: 0.7311\n",
      "Epoch 56/200, Loss: 0.7521\n",
      "Epoch 57/200, Loss: 0.7688\n",
      "Epoch 58/200, Loss: 0.7227\n",
      "Epoch 59/200, Loss: 0.7827\n",
      "Epoch 60/200, Loss: 0.7113\n",
      "Epoch 61/200, Loss: 0.7200\n",
      "Epoch 62/200, Loss: 0.7245\n",
      "Epoch 63/200, Loss: 0.6914\n",
      "Epoch 64/200, Loss: 0.6764\n",
      "Epoch 65/200, Loss: 0.7323\n",
      "Epoch 66/200, Loss: 0.7068\n",
      "Epoch 67/200, Loss: 0.6850\n",
      "Epoch 68/200, Loss: 0.6952\n",
      "Epoch 69/200, Loss: 0.6509\n",
      "Epoch 70/200, Loss: 0.6345\n",
      "Epoch 71/200, Loss: 0.6717\n",
      "Epoch 72/200, Loss: 0.6770\n",
      "Epoch 73/200, Loss: 0.6514\n",
      "Epoch 74/200, Loss: 0.6669\n",
      "Epoch 75/200, Loss: 0.6216\n",
      "Epoch 76/200, Loss: 0.6494\n",
      "Epoch 77/200, Loss: 0.6409\n",
      "Epoch 78/200, Loss: 0.6384\n",
      "Epoch 79/200, Loss: 0.6387\n",
      "Epoch 80/200, Loss: 0.6412\n",
      "Epoch 81/200, Loss: 0.6428\n",
      "Epoch 82/200, Loss: 0.6300\n",
      "Epoch 83/200, Loss: 0.6190\n",
      "Epoch 84/200, Loss: 0.6286\n",
      "Epoch 85/200, Loss: 0.6012\n",
      "Epoch 86/200, Loss: 0.6237\n",
      "Epoch 87/200, Loss: 0.6293\n",
      "Epoch 88/200, Loss: 0.6080\n",
      "Epoch 89/200, Loss: 0.6234\n",
      "Epoch 90/200, Loss: 0.6374\n",
      "Epoch 91/200, Loss: 0.6342\n",
      "Epoch 92/200, Loss: 0.6191\n",
      "Epoch 93/200, Loss: 0.6158\n",
      "Epoch 94/200, Loss: 0.6286\n",
      "Epoch 95/200, Loss: 0.6027\n",
      "Epoch 96/200, Loss: 0.6131\n",
      "Epoch 97/200, Loss: 0.6226\n",
      "Epoch 98/200, Loss: 0.6213\n",
      "Epoch 99/200, Loss: 0.6163\n",
      "Epoch 100/200, Loss: 0.6059\n",
      "Epoch 101/200, Loss: 0.6195\n",
      "Epoch 102/200, Loss: 0.6114\n",
      "Epoch 103/200, Loss: 0.6075\n",
      "Epoch 104/200, Loss: 0.6151\n",
      "Epoch 105/200, Loss: 0.6127\n",
      "Epoch 106/200, Loss: 0.6130\n",
      "Epoch 107/200, Loss: 0.6007\n",
      "Epoch 108/200, Loss: 0.6129\n",
      "Epoch 109/200, Loss: 0.6156\n",
      "Epoch 110/200, Loss: 0.6120\n",
      "Epoch 111/200, Loss: 0.6005\n",
      "Epoch 112/200, Loss: 0.6054\n",
      "Epoch 113/200, Loss: 0.6095\n",
      "Epoch 114/200, Loss: 0.6026\n",
      "Epoch 115/200, Loss: 0.5954\n",
      "Epoch 116/200, Loss: 0.5937\n",
      "Epoch 117/200, Loss: 0.6039\n",
      "Epoch 118/200, Loss: 0.6055\n",
      "Epoch 119/200, Loss: 0.5998\n",
      "Epoch 120/200, Loss: 0.6138\n",
      "Epoch 121/200, Loss: 0.5844\n",
      "Epoch 122/200, Loss: 0.6133\n",
      "Epoch 123/200, Loss: 0.5917\n",
      "Epoch 124/200, Loss: 0.6042\n",
      "Epoch 125/200, Loss: 0.6008\n",
      "Epoch 126/200, Loss: 0.6011\n",
      "Epoch 127/200, Loss: 0.5841\n",
      "Epoch 128/200, Loss: 0.5970\n",
      "Epoch 129/200, Loss: 0.6063\n",
      "Epoch 130/200, Loss: 0.6022\n",
      "Epoch 131/200, Loss: 0.5899\n",
      "Epoch 132/200, Loss: 0.6048\n",
      "Epoch 133/200, Loss: 0.5911\n",
      "Epoch 134/200, Loss: 0.5978\n",
      "Epoch 135/200, Loss: 0.5908\n",
      "Epoch 136/200, Loss: 0.5994\n",
      "Epoch 137/200, Loss: 0.6039\n",
      "Epoch 138/200, Loss: 0.6045\n",
      "Epoch 139/200, Loss: 0.6080\n",
      "Epoch 140/200, Loss: 0.5936\n",
      "Epoch 141/200, Loss: 0.5931\n",
      "Epoch 142/200, Loss: 0.6002\n",
      "Epoch 143/200, Loss: 0.6108\n",
      "Epoch 144/200, Loss: 0.5906\n",
      "Epoch 145/200, Loss: 0.5888\n",
      "Epoch 146/200, Loss: 0.5885\n",
      "Epoch 147/200, Loss: 0.5928\n",
      "Epoch 148/200, Loss: 0.5949\n",
      "Epoch 149/200, Loss: 0.5956\n",
      "Epoch 150/200, Loss: 0.5925\n",
      "Epoch 151/200, Loss: 0.5829\n",
      "Epoch 152/200, Loss: 0.5830\n",
      "Epoch 153/200, Loss: 0.5931\n",
      "Epoch 154/200, Loss: 0.5957\n",
      "Epoch 155/200, Loss: 0.5856\n",
      "Epoch 156/200, Loss: 0.5890\n",
      "Epoch 157/200, Loss: 0.5964\n",
      "Epoch 158/200, Loss: 0.5907\n",
      "Epoch 159/200, Loss: 0.5916\n",
      "Epoch 160/200, Loss: 0.5844\n",
      "Epoch 161/200, Loss: 0.5861\n",
      "Epoch 162/200, Loss: 0.5867\n",
      "Epoch 163/200, Loss: 0.5857\n",
      "Epoch 164/200, Loss: 0.5829\n",
      "Epoch 165/200, Loss: 0.5755\n",
      "Epoch 166/200, Loss: 0.5941\n",
      "Epoch 167/200, Loss: 0.5903\n",
      "Epoch 168/200, Loss: 0.5850\n",
      "Epoch 169/200, Loss: 0.5865\n",
      "Epoch 170/200, Loss: 0.5836\n",
      "Epoch 171/200, Loss: 0.5776\n",
      "Epoch 172/200, Loss: 0.5789\n",
      "Epoch 173/200, Loss: 0.5849\n",
      "Epoch 174/200, Loss: 0.5815\n",
      "Epoch 175/200, Loss: 0.5697\n",
      "Epoch 176/200, Loss: 0.5677\n",
      "Epoch 177/200, Loss: 0.5749\n",
      "Epoch 178/200, Loss: 0.5684\n",
      "Epoch 179/200, Loss: 0.5780\n",
      "Epoch 180/200, Loss: 0.5656\n",
      "Epoch 181/200, Loss: 0.5782\n",
      "Epoch 182/200, Loss: 0.5805\n",
      "Epoch 183/200, Loss: 0.5682\n",
      "Epoch 184/200, Loss: 0.5670\n",
      "Epoch 185/200, Loss: 0.5800\n",
      "Epoch 186/200, Loss: 0.5733\n",
      "Epoch 187/200, Loss: 0.5797\n",
      "Epoch 188/200, Loss: 0.5675\n",
      "Epoch 189/200, Loss: 0.5601\n",
      "Epoch 190/200, Loss: 0.5669\n",
      "Epoch 191/200, Loss: 0.5656\n",
      "Epoch 192/200, Loss: 0.5686\n",
      "Epoch 193/200, Loss: 0.5669\n",
      "Epoch 194/200, Loss: 0.5628\n",
      "Epoch 195/200, Loss: 0.5696\n",
      "Epoch 196/200, Loss: 0.5595\n",
      "Epoch 197/200, Loss: 0.5618\n",
      "Epoch 198/200, Loss: 0.5535\n",
      "Epoch 199/200, Loss: 0.5510\n",
      "Epoch 200/200, Loss: 0.5572\n",
      "‚úÖ Ë®ìÁ∑¥Ê≠£Á¢∫ÁéáÔºö72.05%\n",
      "‚úÖ Ê∏¨Ë©¶Ê≠£Á¢∫ÁéáÔºö73.73%\n",
      "üíæ Ê®°ÂûãÂ∑≤ÂÑ≤Â≠òÁÇ∫ model_strike_predictor_debug.pth\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FF\n",
      "features\\Yu_Darvish_FF_videos_4S\\train ÂÖ±Êúâ 156 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FS\n",
      "features\\Yu_Darvish_FS_videos_4S\\train ÂÖ±Êúâ 149 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_SL\n",
      "features\\Yu_Darvish_SL_videos_4S\\train ÂÖ±Êúâ 153 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FF\n",
      "features\\Yu_Darvish_FF_videos_4S\\test ÂÖ±Êúâ 40 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FS\n",
      "features\\Yu_Darvish_FS_videos_4S\\test ÂÖ±Êúâ 38 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_SL\n",
      "features\\Yu_Darvish_SL_videos_4S\\test ÂÖ±Êúâ 40 ÂÄã .npy Ê™îÊ°à\n",
      "(458, 4000)\n",
      "üìä Ë®ìÁ∑¥Ë≥áÊñôÁ≠ÜÊï∏Ôºö458ÔºåÊ∏¨Ë©¶Ë≥áÊñôÁ≠ÜÊï∏Ôºö118\n",
      "Epoch 1/200, Loss: 1.9224\n",
      "Epoch 2/200, Loss: 63.8089\n",
      "Epoch 3/200, Loss: 3.1748\n",
      "Epoch 4/200, Loss: 16.1670\n",
      "Epoch 5/200, Loss: 15.2468\n",
      "Epoch 6/200, Loss: 9.5732\n",
      "Epoch 7/200, Loss: 4.1526\n",
      "Epoch 8/200, Loss: 10.0277\n",
      "Epoch 9/200, Loss: 7.5006\n",
      "Epoch 10/200, Loss: 2.9779\n",
      "Epoch 11/200, Loss: 4.5145\n",
      "Epoch 12/200, Loss: 5.8975\n",
      "Epoch 13/200, Loss: 6.5958\n",
      "Epoch 14/200, Loss: 4.4712\n",
      "Epoch 15/200, Loss: 2.6592\n",
      "Epoch 16/200, Loss: 3.5157\n",
      "Epoch 17/200, Loss: 4.2482\n",
      "Epoch 18/200, Loss: 3.0335\n",
      "Epoch 19/200, Loss: 1.8869\n",
      "Epoch 20/200, Loss: 2.6362\n",
      "Epoch 21/200, Loss: 2.8972\n",
      "Epoch 22/200, Loss: 2.7521\n",
      "Epoch 23/200, Loss: 2.2264\n",
      "Epoch 24/200, Loss: 1.6474\n",
      "Epoch 25/200, Loss: 1.8004\n",
      "Epoch 26/200, Loss: 2.1717\n",
      "Epoch 27/200, Loss: 1.7426\n",
      "Epoch 28/200, Loss: 1.2274\n",
      "Epoch 29/200, Loss: 1.4008\n",
      "Epoch 30/200, Loss: 1.5374\n",
      "Epoch 31/200, Loss: 1.4187\n",
      "Epoch 32/200, Loss: 1.2803\n",
      "Epoch 33/200, Loss: 1.0384\n",
      "Epoch 34/200, Loss: 1.1523\n",
      "Epoch 35/200, Loss: 1.2672\n",
      "Epoch 36/200, Loss: 1.1744\n",
      "Epoch 37/200, Loss: 1.0327\n",
      "Epoch 38/200, Loss: 0.9002\n",
      "Epoch 39/200, Loss: 0.9820\n",
      "Epoch 40/200, Loss: 0.9663\n",
      "Epoch 41/200, Loss: 1.0070\n",
      "Epoch 42/200, Loss: 0.7992\n",
      "Epoch 43/200, Loss: 0.8853\n",
      "Epoch 44/200, Loss: 0.8326\n",
      "Epoch 45/200, Loss: 0.7968\n",
      "Epoch 46/200, Loss: 0.7105\n",
      "Epoch 47/200, Loss: 0.7383\n",
      "Epoch 48/200, Loss: 0.7337\n",
      "Epoch 49/200, Loss: 0.7233\n",
      "Epoch 50/200, Loss: 0.6907\n",
      "Epoch 51/200, Loss: 0.7150\n",
      "Epoch 52/200, Loss: 0.6973\n",
      "Epoch 53/200, Loss: 0.6564\n",
      "Epoch 54/200, Loss: 0.6650\n",
      "Epoch 55/200, Loss: 0.6352\n",
      "Epoch 56/200, Loss: 0.6797\n",
      "Epoch 57/200, Loss: 0.6419\n",
      "Epoch 58/200, Loss: 0.6718\n",
      "Epoch 59/200, Loss: 0.6527\n",
      "Epoch 60/200, Loss: 0.6406\n",
      "Epoch 61/200, Loss: 0.6260\n",
      "Epoch 62/200, Loss: 0.6188\n",
      "Epoch 63/200, Loss: 0.6429\n",
      "Epoch 64/200, Loss: 0.6193\n",
      "Epoch 65/200, Loss: 0.6360\n",
      "Epoch 66/200, Loss: 0.6230\n",
      "Epoch 67/200, Loss: 0.6192\n",
      "Epoch 68/200, Loss: 0.6292\n",
      "Epoch 69/200, Loss: 0.6211\n",
      "Epoch 70/200, Loss: 0.6131\n",
      "Epoch 71/200, Loss: 0.6409\n",
      "Epoch 72/200, Loss: 0.6126\n",
      "Epoch 73/200, Loss: 0.6141\n",
      "Epoch 74/200, Loss: 0.6185\n",
      "Epoch 75/200, Loss: 0.6106\n",
      "Epoch 76/200, Loss: 0.6296\n",
      "Epoch 77/200, Loss: 0.6125\n",
      "Epoch 78/200, Loss: 0.6179\n",
      "Epoch 79/200, Loss: 0.6229\n",
      "Epoch 80/200, Loss: 0.6085\n",
      "Epoch 81/200, Loss: 0.6162\n",
      "Epoch 82/200, Loss: 0.6160\n",
      "Epoch 83/200, Loss: 0.6114\n",
      "Epoch 84/200, Loss: 0.6073\n",
      "Epoch 85/200, Loss: 0.6030\n",
      "Epoch 86/200, Loss: 0.6132\n",
      "Epoch 87/200, Loss: 0.6064\n",
      "Epoch 88/200, Loss: 0.6110\n",
      "Epoch 89/200, Loss: 0.6106\n",
      "Epoch 90/200, Loss: 0.6065\n",
      "Epoch 91/200, Loss: 0.6131\n",
      "Epoch 92/200, Loss: 0.6076\n",
      "Epoch 93/200, Loss: 0.6108\n",
      "Epoch 94/200, Loss: 0.6136\n",
      "Epoch 95/200, Loss: 0.6117\n",
      "Epoch 96/200, Loss: 0.6046\n",
      "Epoch 97/200, Loss: 0.6071\n",
      "Epoch 98/200, Loss: 0.6145\n",
      "Epoch 99/200, Loss: 0.6046\n",
      "Epoch 100/200, Loss: 0.6091\n",
      "Epoch 101/200, Loss: 0.6052\n",
      "Epoch 102/200, Loss: 0.6113\n",
      "Epoch 103/200, Loss: 0.6022\n",
      "Epoch 104/200, Loss: 0.6076\n",
      "Epoch 105/200, Loss: 0.6056\n",
      "Epoch 106/200, Loss: 0.6151\n",
      "Epoch 107/200, Loss: 0.6077\n",
      "Epoch 108/200, Loss: 0.6100\n",
      "Epoch 109/200, Loss: 0.6085\n",
      "Epoch 110/200, Loss: 0.6099\n",
      "Epoch 111/200, Loss: 0.6060\n",
      "Epoch 112/200, Loss: 0.6047\n",
      "Epoch 113/200, Loss: 0.6085\n",
      "Epoch 114/200, Loss: 0.6073\n",
      "Epoch 115/200, Loss: 0.5991\n",
      "Epoch 116/200, Loss: 0.6045\n",
      "Epoch 117/200, Loss: 0.6061\n",
      "Epoch 118/200, Loss: 0.6032\n",
      "Epoch 119/200, Loss: 0.6060\n",
      "Epoch 120/200, Loss: 0.6047\n",
      "Epoch 121/200, Loss: 0.6045\n",
      "Epoch 122/200, Loss: 0.6043\n",
      "Epoch 123/200, Loss: 0.6049\n",
      "Epoch 124/200, Loss: 0.6051\n",
      "Epoch 125/200, Loss: 0.6023\n",
      "Epoch 126/200, Loss: 0.6081\n",
      "Epoch 127/200, Loss: 0.6088\n",
      "Epoch 128/200, Loss: 0.6093\n",
      "Epoch 129/200, Loss: 0.5994\n",
      "Epoch 130/200, Loss: 0.6051\n",
      "Epoch 131/200, Loss: 0.6048\n",
      "Epoch 132/200, Loss: 0.6005\n",
      "Epoch 133/200, Loss: 0.6070\n",
      "Epoch 134/200, Loss: 0.5993\n",
      "Epoch 135/200, Loss: 0.6029\n",
      "Epoch 136/200, Loss: 0.6027\n",
      "Epoch 137/200, Loss: 0.6047\n",
      "Epoch 138/200, Loss: 0.6043\n",
      "Epoch 139/200, Loss: 0.6070\n",
      "Epoch 140/200, Loss: 0.6016\n",
      "Epoch 141/200, Loss: 0.6037\n",
      "Epoch 142/200, Loss: 0.6049\n",
      "Epoch 143/200, Loss: 0.6028\n",
      "Epoch 144/200, Loss: 0.6034\n",
      "Epoch 145/200, Loss: 0.6078\n",
      "Epoch 146/200, Loss: 0.6039\n",
      "Epoch 147/200, Loss: 0.6040\n",
      "Epoch 148/200, Loss: 0.5980\n",
      "Epoch 149/200, Loss: 0.6044\n",
      "Epoch 150/200, Loss: 0.5949\n",
      "Epoch 151/200, Loss: 0.6078\n",
      "Epoch 152/200, Loss: 0.6053\n",
      "Epoch 153/200, Loss: 0.5965\n",
      "Epoch 154/200, Loss: 0.5974\n",
      "Epoch 155/200, Loss: 0.6038\n",
      "Epoch 156/200, Loss: 0.6042\n",
      "Epoch 157/200, Loss: 0.6051\n",
      "Epoch 158/200, Loss: 0.6010\n",
      "Epoch 159/200, Loss: 0.6062\n",
      "Epoch 160/200, Loss: 0.6023\n",
      "Epoch 161/200, Loss: 0.5962\n",
      "Epoch 162/200, Loss: 0.5985\n",
      "Epoch 163/200, Loss: 0.6037\n",
      "Epoch 164/200, Loss: 0.6046\n",
      "Epoch 165/200, Loss: 0.6063\n",
      "Epoch 166/200, Loss: 0.6015\n",
      "Epoch 167/200, Loss: 0.5955\n",
      "Epoch 168/200, Loss: 0.5960\n",
      "Epoch 169/200, Loss: 0.6052\n",
      "Epoch 170/200, Loss: 0.6040\n",
      "Epoch 171/200, Loss: 0.5954\n",
      "Epoch 172/200, Loss: 0.5937\n",
      "Epoch 173/200, Loss: 0.6094\n",
      "Epoch 174/200, Loss: 0.6029\n",
      "Epoch 175/200, Loss: 0.6001\n",
      "Epoch 176/200, Loss: 0.6000\n",
      "Epoch 177/200, Loss: 0.6013\n",
      "Epoch 178/200, Loss: 0.6030\n",
      "Epoch 179/200, Loss: 0.6004\n",
      "Epoch 180/200, Loss: 0.5943\n",
      "Epoch 181/200, Loss: 0.5982\n",
      "Epoch 182/200, Loss: 0.5949\n",
      "Epoch 183/200, Loss: 0.6010\n",
      "Epoch 184/200, Loss: 0.5970\n",
      "Epoch 185/200, Loss: 0.5950\n",
      "Epoch 186/200, Loss: 0.5961\n",
      "Epoch 187/200, Loss: 0.6017\n",
      "Epoch 188/200, Loss: 0.5965\n",
      "Epoch 189/200, Loss: 0.5949\n",
      "Epoch 190/200, Loss: 0.5930\n",
      "Epoch 191/200, Loss: 0.5887\n",
      "Epoch 192/200, Loss: 0.5902\n",
      "Epoch 193/200, Loss: 0.5994\n",
      "Epoch 194/200, Loss: 0.5856\n",
      "Epoch 195/200, Loss: 0.5923\n",
      "Epoch 196/200, Loss: 0.5934\n",
      "Epoch 197/200, Loss: 0.5947\n",
      "Epoch 198/200, Loss: 0.5969\n",
      "Epoch 199/200, Loss: 0.5974\n",
      "Epoch 200/200, Loss: 0.6012\n",
      "‚úÖ Ë®ìÁ∑¥Ê≠£Á¢∫ÁéáÔºö70.74%\n",
      "‚úÖ Ê∏¨Ë©¶Ê≠£Á¢∫ÁéáÔºö75.42%\n",
      "üíæ Ê®°ÂûãÂ∑≤ÂÑ≤Â≠òÁÇ∫ model_strike_predictor_debug.pth\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FF\n",
      "features\\Yu_Darvish_FF_videos_4S\\train ÂÖ±Êúâ 156 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FS\n",
      "features\\Yu_Darvish_FS_videos_4S\\train ÂÖ±Êúâ 149 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_SL\n",
      "features\\Yu_Darvish_SL_videos_4S\\train ÂÖ±Êúâ 153 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FF\n",
      "features\\Yu_Darvish_FF_videos_4S\\test ÂÖ±Êúâ 40 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FS\n",
      "features\\Yu_Darvish_FS_videos_4S\\test ÂÖ±Êúâ 38 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_SL\n",
      "features\\Yu_Darvish_SL_videos_4S\\test ÂÖ±Êúâ 40 ÂÄã .npy Ê™îÊ°à\n",
      "(458, 4000)\n",
      "üìä Ë®ìÁ∑¥Ë≥áÊñôÁ≠ÜÊï∏Ôºö458ÔºåÊ∏¨Ë©¶Ë≥áÊñôÁ≠ÜÊï∏Ôºö118\n",
      "Epoch 1/200, Loss: 6.3570\n",
      "Epoch 2/200, Loss: 38.0718\n",
      "Epoch 3/200, Loss: 2.6596\n",
      "Epoch 4/200, Loss: 6.8009\n",
      "Epoch 5/200, Loss: 4.1263\n",
      "Epoch 6/200, Loss: 4.7889\n",
      "Epoch 7/200, Loss: 3.9045\n",
      "Epoch 8/200, Loss: 3.3957\n",
      "Epoch 9/200, Loss: 4.4750\n",
      "Epoch 10/200, Loss: 3.5405\n",
      "Epoch 11/200, Loss: 2.2848\n",
      "Epoch 12/200, Loss: 3.6789\n",
      "Epoch 13/200, Loss: 2.5421\n",
      "Epoch 14/200, Loss: 2.1825\n",
      "Epoch 15/200, Loss: 2.4124\n",
      "Epoch 16/200, Loss: 2.3755\n",
      "Epoch 17/200, Loss: 1.9671\n",
      "Epoch 18/200, Loss: 1.8845\n",
      "Epoch 19/200, Loss: 2.2704\n",
      "Epoch 20/200, Loss: 1.6956\n",
      "Epoch 21/200, Loss: 1.5609\n",
      "Epoch 22/200, Loss: 1.8589\n",
      "Epoch 23/200, Loss: 1.8196\n",
      "Epoch 24/200, Loss: 1.2179\n",
      "Epoch 25/200, Loss: 1.2047\n",
      "Epoch 26/200, Loss: 1.4190\n",
      "Epoch 27/200, Loss: 1.3287\n",
      "Epoch 28/200, Loss: 1.0638\n",
      "Epoch 29/200, Loss: 1.1253\n",
      "Epoch 30/200, Loss: 1.1591\n",
      "Epoch 31/200, Loss: 1.0443\n",
      "Epoch 32/200, Loss: 0.9298\n",
      "Epoch 33/200, Loss: 0.9252\n",
      "Epoch 34/200, Loss: 0.9306\n",
      "Epoch 35/200, Loss: 0.8741\n",
      "Epoch 36/200, Loss: 0.8904\n",
      "Epoch 37/200, Loss: 0.9068\n",
      "Epoch 38/200, Loss: 0.9023\n",
      "Epoch 39/200, Loss: 0.7804\n",
      "Epoch 40/200, Loss: 0.7452\n",
      "Epoch 41/200, Loss: 0.7866\n",
      "Epoch 42/200, Loss: 0.7875\n",
      "Epoch 43/200, Loss: 0.7544\n",
      "Epoch 44/200, Loss: 0.6850\n",
      "Epoch 45/200, Loss: 0.6827\n",
      "Epoch 46/200, Loss: 0.6565\n",
      "Epoch 47/200, Loss: 0.7168\n",
      "Epoch 48/200, Loss: 0.7455\n",
      "Epoch 49/200, Loss: 0.6813\n",
      "Epoch 50/200, Loss: 0.6582\n",
      "Epoch 51/200, Loss: 0.6545\n",
      "Epoch 52/200, Loss: 0.6212\n",
      "Epoch 53/200, Loss: 0.6430\n",
      "Epoch 54/200, Loss: 0.6446\n",
      "Epoch 55/200, Loss: 0.6326\n",
      "Epoch 56/200, Loss: 0.6166\n",
      "Epoch 57/200, Loss: 0.6209\n",
      "Epoch 58/200, Loss: 0.6232\n",
      "Epoch 59/200, Loss: 0.6258\n",
      "Epoch 60/200, Loss: 0.6110\n",
      "Epoch 61/200, Loss: 0.6275\n",
      "Epoch 62/200, Loss: 0.6228\n",
      "Epoch 63/200, Loss: 0.6229\n",
      "Epoch 64/200, Loss: 0.6026\n",
      "Epoch 65/200, Loss: 0.6144\n",
      "Epoch 66/200, Loss: 0.6115\n",
      "Epoch 67/200, Loss: 0.6108\n",
      "Epoch 68/200, Loss: 0.6154\n",
      "Epoch 69/200, Loss: 0.6190\n",
      "Epoch 70/200, Loss: 0.6138\n",
      "Epoch 71/200, Loss: 0.6269\n",
      "Epoch 72/200, Loss: 0.6069\n",
      "Epoch 73/200, Loss: 0.6192\n",
      "Epoch 74/200, Loss: 0.6093\n",
      "Epoch 75/200, Loss: 0.6035\n",
      "Epoch 76/200, Loss: 0.6105\n",
      "Epoch 77/200, Loss: 0.6126\n",
      "Epoch 78/200, Loss: 0.6153\n",
      "Epoch 79/200, Loss: 0.6076\n",
      "Epoch 80/200, Loss: 0.6192\n",
      "Epoch 81/200, Loss: 0.6153\n",
      "Epoch 82/200, Loss: 0.6041\n",
      "Epoch 83/200, Loss: 0.6106\n",
      "Epoch 84/200, Loss: 0.6025\n",
      "Epoch 85/200, Loss: 0.6033\n",
      "Epoch 86/200, Loss: 0.6082\n",
      "Epoch 87/200, Loss: 0.6133\n",
      "Epoch 88/200, Loss: 0.6021\n",
      "Epoch 89/200, Loss: 0.6003\n",
      "Epoch 90/200, Loss: 0.5934\n",
      "Epoch 91/200, Loss: 0.6169\n",
      "Epoch 92/200, Loss: 0.6081\n",
      "Epoch 93/200, Loss: 0.5920\n",
      "Epoch 94/200, Loss: 0.6162\n",
      "Epoch 95/200, Loss: 0.5872\n",
      "Epoch 96/200, Loss: 0.6012\n",
      "Epoch 97/200, Loss: 0.6026\n",
      "Epoch 98/200, Loss: 0.6018\n",
      "Epoch 99/200, Loss: 0.6010\n",
      "Epoch 100/200, Loss: 0.5950\n",
      "Epoch 101/200, Loss: 0.6066\n",
      "Epoch 102/200, Loss: 0.6021\n",
      "Epoch 103/200, Loss: 0.5911\n",
      "Epoch 104/200, Loss: 0.6045\n",
      "Epoch 105/200, Loss: 0.6074\n",
      "Epoch 106/200, Loss: 0.5954\n",
      "Epoch 107/200, Loss: 0.6086\n",
      "Epoch 108/200, Loss: 0.6044\n",
      "Epoch 109/200, Loss: 0.5951\n",
      "Epoch 110/200, Loss: 0.6008\n",
      "Epoch 111/200, Loss: 0.6001\n",
      "Epoch 112/200, Loss: 0.6025\n",
      "Epoch 113/200, Loss: 0.5949\n",
      "Epoch 114/200, Loss: 0.5930\n",
      "Epoch 115/200, Loss: 0.5865\n",
      "Epoch 116/200, Loss: 0.5943\n",
      "Epoch 117/200, Loss: 0.5943\n",
      "Epoch 118/200, Loss: 0.5917\n",
      "Epoch 119/200, Loss: 0.5910\n",
      "Epoch 120/200, Loss: 0.5930\n",
      "Epoch 121/200, Loss: 0.5951\n",
      "Epoch 122/200, Loss: 0.5919\n",
      "Epoch 123/200, Loss: 0.5939\n",
      "Epoch 124/200, Loss: 0.5975\n",
      "Epoch 125/200, Loss: 0.6027\n",
      "Epoch 126/200, Loss: 0.5963\n",
      "Epoch 127/200, Loss: 0.5961\n",
      "Epoch 128/200, Loss: 0.6020\n",
      "Epoch 129/200, Loss: 0.5953\n",
      "Epoch 130/200, Loss: 0.5951\n",
      "Epoch 131/200, Loss: 0.5939\n",
      "Epoch 132/200, Loss: 0.5913\n",
      "Epoch 133/200, Loss: 0.5959\n",
      "Epoch 134/200, Loss: 0.5912\n",
      "Epoch 135/200, Loss: 0.5754\n",
      "Epoch 136/200, Loss: 0.6052\n",
      "Epoch 137/200, Loss: 0.5906\n",
      "Epoch 138/200, Loss: 0.5760\n",
      "Epoch 139/200, Loss: 0.5813\n",
      "Epoch 140/200, Loss: 0.5993\n",
      "Epoch 141/200, Loss: 0.5791\n",
      "Epoch 142/200, Loss: 0.5907\n",
      "Epoch 143/200, Loss: 0.5896\n",
      "Epoch 144/200, Loss: 0.5913\n",
      "Epoch 145/200, Loss: 0.5800\n",
      "Epoch 146/200, Loss: 0.5963\n",
      "Epoch 147/200, Loss: 0.5831\n",
      "Epoch 148/200, Loss: 0.5824\n",
      "Epoch 149/200, Loss: 0.5784\n",
      "Epoch 150/200, Loss: 0.5842\n",
      "Epoch 151/200, Loss: 0.5891\n",
      "Epoch 152/200, Loss: 0.5928\n",
      "Epoch 153/200, Loss: 0.5722\n",
      "Epoch 154/200, Loss: 0.5838\n",
      "Epoch 155/200, Loss: 0.5774\n",
      "Epoch 156/200, Loss: 0.5793\n",
      "Epoch 157/200, Loss: 0.5788\n",
      "Epoch 158/200, Loss: 0.5652\n",
      "Epoch 159/200, Loss: 0.5779\n",
      "Epoch 160/200, Loss: 0.5660\n",
      "Epoch 161/200, Loss: 0.5955\n",
      "Epoch 162/200, Loss: 0.5704\n",
      "Epoch 163/200, Loss: 0.5736\n",
      "Epoch 164/200, Loss: 0.5764\n",
      "Epoch 165/200, Loss: 0.5735\n",
      "Epoch 166/200, Loss: 0.5641\n",
      "Epoch 167/200, Loss: 0.5745\n",
      "Epoch 168/200, Loss: 0.5730\n",
      "Epoch 169/200, Loss: 0.5676\n",
      "Epoch 170/200, Loss: 0.5679\n",
      "Epoch 171/200, Loss: 0.5770\n",
      "Epoch 172/200, Loss: 0.5640\n",
      "Epoch 173/200, Loss: 0.5730\n",
      "Epoch 174/200, Loss: 0.5722\n",
      "Epoch 175/200, Loss: 0.5641\n",
      "Epoch 176/200, Loss: 0.5585\n",
      "Epoch 177/200, Loss: 0.5754\n",
      "Epoch 178/200, Loss: 0.5950\n",
      "Epoch 179/200, Loss: 0.5754\n",
      "Epoch 180/200, Loss: 0.5609\n",
      "Epoch 181/200, Loss: 0.5603\n",
      "Epoch 182/200, Loss: 0.5675\n",
      "Epoch 183/200, Loss: 0.5579\n",
      "Epoch 184/200, Loss: 0.5918\n",
      "Epoch 185/200, Loss: 0.5672\n",
      "Epoch 186/200, Loss: 0.5654\n",
      "Epoch 187/200, Loss: 0.5612\n",
      "Epoch 188/200, Loss: 0.5668\n",
      "Epoch 189/200, Loss: 0.5661\n",
      "Epoch 190/200, Loss: 0.5688\n",
      "Epoch 191/200, Loss: 0.5572\n",
      "Epoch 192/200, Loss: 0.5663\n",
      "Epoch 193/200, Loss: 0.5493\n",
      "Epoch 194/200, Loss: 0.5415\n",
      "Epoch 195/200, Loss: 0.5711\n",
      "Epoch 196/200, Loss: 0.5509\n",
      "Epoch 197/200, Loss: 0.5587\n",
      "Epoch 198/200, Loss: 0.5579\n",
      "Epoch 199/200, Loss: 0.5464\n",
      "Epoch 200/200, Loss: 0.5570\n",
      "‚úÖ Ë®ìÁ∑¥Ê≠£Á¢∫ÁéáÔºö74.89%\n",
      "‚úÖ Ê∏¨Ë©¶Ê≠£Á¢∫ÁéáÔºö66.95%\n",
      "üíæ Ê®°ÂûãÂ∑≤ÂÑ≤Â≠òÁÇ∫ model_strike_predictor_debug.pth\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FF\n",
      "features\\Yu_Darvish_FF_videos_4S\\train ÂÖ±Êúâ 156 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FS\n",
      "features\\Yu_Darvish_FS_videos_4S\\train ÂÖ±Êúâ 149 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_SL\n",
      "features\\Yu_Darvish_SL_videos_4S\\train ÂÖ±Êúâ 153 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FF\n",
      "features\\Yu_Darvish_FF_videos_4S\\test ÂÖ±Êúâ 40 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FS\n",
      "features\\Yu_Darvish_FS_videos_4S\\test ÂÖ±Êúâ 38 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_SL\n",
      "features\\Yu_Darvish_SL_videos_4S\\test ÂÖ±Êúâ 40 ÂÄã .npy Ê™îÊ°à\n",
      "(458, 4000)\n",
      "üìä Ë®ìÁ∑¥Ë≥áÊñôÁ≠ÜÊï∏Ôºö458ÔºåÊ∏¨Ë©¶Ë≥áÊñôÁ≠ÜÊï∏Ôºö118\n",
      "Epoch 1/200, Loss: 2.0546\n",
      "Epoch 2/200, Loss: 45.3325\n",
      "Epoch 3/200, Loss: 19.1132\n",
      "Epoch 4/200, Loss: 6.7618\n",
      "Epoch 5/200, Loss: 3.5618\n",
      "Epoch 6/200, Loss: 4.8802\n",
      "Epoch 7/200, Loss: 4.7876\n",
      "Epoch 8/200, Loss: 3.0460\n",
      "Epoch 9/200, Loss: 4.2429\n",
      "Epoch 10/200, Loss: 2.9257\n",
      "Epoch 11/200, Loss: 2.7914\n",
      "Epoch 12/200, Loss: 3.8324\n",
      "Epoch 13/200, Loss: 3.1110\n",
      "Epoch 14/200, Loss: 2.6601\n",
      "Epoch 15/200, Loss: 2.6862\n",
      "Epoch 16/200, Loss: 2.8805\n",
      "Epoch 17/200, Loss: 1.9123\n",
      "Epoch 18/200, Loss: 1.9077\n",
      "Epoch 19/200, Loss: 2.4261\n",
      "Epoch 20/200, Loss: 1.9797\n",
      "Epoch 21/200, Loss: 1.4506\n",
      "Epoch 22/200, Loss: 1.6227\n",
      "Epoch 23/200, Loss: 1.5953\n",
      "Epoch 24/200, Loss: 1.2671\n",
      "Epoch 25/200, Loss: 1.3358\n",
      "Epoch 26/200, Loss: 1.4235\n",
      "Epoch 27/200, Loss: 1.3611\n",
      "Epoch 28/200, Loss: 1.0969\n",
      "Epoch 29/200, Loss: 1.1372\n",
      "Epoch 30/200, Loss: 1.1368\n",
      "Epoch 31/200, Loss: 1.0950\n",
      "Epoch 32/200, Loss: 0.9900\n",
      "Epoch 33/200, Loss: 1.0676\n",
      "Epoch 34/200, Loss: 0.9456\n",
      "Epoch 35/200, Loss: 0.9074\n",
      "Epoch 36/200, Loss: 0.8618\n",
      "Epoch 37/200, Loss: 0.8416\n",
      "Epoch 38/200, Loss: 0.7969\n",
      "Epoch 39/200, Loss: 0.8535\n",
      "Epoch 40/200, Loss: 0.8044\n",
      "Epoch 41/200, Loss: 0.7960\n",
      "Epoch 42/200, Loss: 0.7624\n",
      "Epoch 43/200, Loss: 0.6668\n",
      "Epoch 44/200, Loss: 0.6828\n",
      "Epoch 45/200, Loss: 0.7211\n",
      "Epoch 46/200, Loss: 0.7444\n",
      "Epoch 47/200, Loss: 0.6590\n",
      "Epoch 48/200, Loss: 0.6841\n",
      "Epoch 49/200, Loss: 0.7029\n",
      "Epoch 50/200, Loss: 0.6955\n",
      "Epoch 51/200, Loss: 0.6790\n",
      "Epoch 52/200, Loss: 0.6760\n",
      "Epoch 53/200, Loss: 0.7005\n",
      "Epoch 54/200, Loss: 0.6288\n",
      "Epoch 55/200, Loss: 0.6467\n",
      "Epoch 56/200, Loss: 0.6322\n",
      "Epoch 57/200, Loss: 0.6171\n",
      "Epoch 58/200, Loss: 0.6112\n",
      "Epoch 59/200, Loss: 0.6546\n",
      "Epoch 60/200, Loss: 0.6387\n",
      "Epoch 61/200, Loss: 0.6290\n",
      "Epoch 62/200, Loss: 0.6207\n",
      "Epoch 63/200, Loss: 0.6267\n",
      "Epoch 64/200, Loss: 0.6376\n",
      "Epoch 65/200, Loss: 0.6131\n",
      "Epoch 66/200, Loss: 0.6258\n",
      "Epoch 67/200, Loss: 0.6260\n",
      "Epoch 68/200, Loss: 0.6208\n",
      "Epoch 69/200, Loss: 0.6197\n",
      "Epoch 70/200, Loss: 0.6278\n",
      "Epoch 71/200, Loss: 0.6250\n",
      "Epoch 72/200, Loss: 0.6157\n",
      "Epoch 73/200, Loss: 0.6088\n",
      "Epoch 74/200, Loss: 0.6222\n",
      "Epoch 75/200, Loss: 0.6131\n",
      "Epoch 76/200, Loss: 0.6034\n",
      "Epoch 77/200, Loss: 0.6020\n",
      "Epoch 78/200, Loss: 0.6254\n",
      "Epoch 79/200, Loss: 0.6148\n",
      "Epoch 80/200, Loss: 0.6239\n",
      "Epoch 81/200, Loss: 0.6021\n",
      "Epoch 82/200, Loss: 0.6019\n",
      "Epoch 83/200, Loss: 0.6031\n",
      "Epoch 84/200, Loss: 0.5992\n",
      "Epoch 85/200, Loss: 0.6209\n",
      "Epoch 86/200, Loss: 0.6074\n",
      "Epoch 87/200, Loss: 0.6096\n",
      "Epoch 88/200, Loss: 0.6061\n",
      "Epoch 89/200, Loss: 0.6061\n",
      "Epoch 90/200, Loss: 0.5990\n",
      "Epoch 91/200, Loss: 0.5983\n",
      "Epoch 92/200, Loss: 0.5878\n",
      "Epoch 93/200, Loss: 0.6034\n",
      "Epoch 94/200, Loss: 0.5901\n",
      "Epoch 95/200, Loss: 0.6016\n",
      "Epoch 96/200, Loss: 0.6006\n",
      "Epoch 97/200, Loss: 0.5916\n",
      "Epoch 98/200, Loss: 0.5868\n",
      "Epoch 99/200, Loss: 0.5911\n",
      "Epoch 100/200, Loss: 0.6068\n",
      "Epoch 101/200, Loss: 0.5950\n",
      "Epoch 102/200, Loss: 0.5972\n",
      "Epoch 103/200, Loss: 0.5890\n",
      "Epoch 104/200, Loss: 0.5895\n",
      "Epoch 105/200, Loss: 0.5921\n",
      "Epoch 106/200, Loss: 0.5899\n",
      "Epoch 107/200, Loss: 0.5924\n",
      "Epoch 108/200, Loss: 0.5810\n",
      "Epoch 109/200, Loss: 0.6002\n",
      "Epoch 110/200, Loss: 0.5985\n",
      "Epoch 111/200, Loss: 0.5901\n",
      "Epoch 112/200, Loss: 0.5880\n",
      "Epoch 113/200, Loss: 0.5755\n",
      "Epoch 114/200, Loss: 0.5934\n",
      "Epoch 115/200, Loss: 0.5943\n",
      "Epoch 116/200, Loss: 0.5972\n",
      "Epoch 117/200, Loss: 0.5871\n",
      "Epoch 118/200, Loss: 0.5955\n",
      "Epoch 119/200, Loss: 0.5944\n",
      "Epoch 120/200, Loss: 0.5959\n",
      "Epoch 121/200, Loss: 0.5903\n",
      "Epoch 122/200, Loss: 0.5802\n",
      "Epoch 123/200, Loss: 0.5903\n",
      "Epoch 124/200, Loss: 0.5802\n",
      "Epoch 125/200, Loss: 0.5851\n",
      "Epoch 126/200, Loss: 0.6023\n",
      "Epoch 127/200, Loss: 0.5829\n",
      "Epoch 128/200, Loss: 0.5915\n",
      "Epoch 129/200, Loss: 0.5895\n",
      "Epoch 130/200, Loss: 0.5802\n",
      "Epoch 131/200, Loss: 0.5795\n",
      "Epoch 132/200, Loss: 0.5764\n",
      "Epoch 133/200, Loss: 0.5782\n",
      "Epoch 134/200, Loss: 0.5749\n",
      "Epoch 135/200, Loss: 0.5768\n",
      "Epoch 136/200, Loss: 0.5746\n",
      "Epoch 137/200, Loss: 0.5826\n",
      "Epoch 138/200, Loss: 0.5798\n",
      "Epoch 139/200, Loss: 0.5805\n",
      "Epoch 140/200, Loss: 0.5753\n",
      "Epoch 141/200, Loss: 0.5711\n",
      "Epoch 142/200, Loss: 0.5681\n",
      "Epoch 143/200, Loss: 0.5749\n",
      "Epoch 144/200, Loss: 0.5585\n",
      "Epoch 145/200, Loss: 0.5688\n",
      "Epoch 146/200, Loss: 0.5904\n",
      "Epoch 147/200, Loss: 0.5619\n",
      "Epoch 148/200, Loss: 0.5766\n",
      "Epoch 149/200, Loss: 0.5653\n",
      "Epoch 150/200, Loss: 0.5907\n",
      "Epoch 151/200, Loss: 0.5880\n",
      "Epoch 152/200, Loss: 0.5718\n",
      "Epoch 153/200, Loss: 0.5871\n",
      "Epoch 154/200, Loss: 0.5673\n",
      "Epoch 155/200, Loss: 0.5764\n",
      "Epoch 156/200, Loss: 0.5841\n",
      "Epoch 157/200, Loss: 0.5821\n",
      "Epoch 158/200, Loss: 0.5793\n",
      "Epoch 159/200, Loss: 0.5907\n",
      "Epoch 160/200, Loss: 0.5852\n",
      "Epoch 161/200, Loss: 0.5778\n",
      "Epoch 162/200, Loss: 0.5872\n",
      "Epoch 163/200, Loss: 0.5730\n",
      "Epoch 164/200, Loss: 0.5649\n",
      "Epoch 165/200, Loss: 0.5606\n",
      "Epoch 166/200, Loss: 0.5490\n",
      "Epoch 167/200, Loss: 0.5667\n",
      "Epoch 168/200, Loss: 0.5763\n",
      "Epoch 169/200, Loss: 0.5513\n",
      "Epoch 170/200, Loss: 0.5932\n",
      "Epoch 171/200, Loss: 0.5643\n",
      "Epoch 172/200, Loss: 0.5632\n",
      "Epoch 173/200, Loss: 0.5671\n",
      "Epoch 174/200, Loss: 0.5791\n",
      "Epoch 175/200, Loss: 0.5511\n",
      "Epoch 176/200, Loss: 0.5594\n",
      "Epoch 177/200, Loss: 0.5558\n",
      "Epoch 178/200, Loss: 0.5641\n",
      "Epoch 179/200, Loss: 0.5545\n",
      "Epoch 180/200, Loss: 0.5736\n",
      "Epoch 181/200, Loss: 0.5999\n",
      "Epoch 182/200, Loss: 0.5720\n",
      "Epoch 183/200, Loss: 0.5723\n",
      "Epoch 184/200, Loss: 0.5763\n",
      "Epoch 185/200, Loss: 0.5754\n",
      "Epoch 186/200, Loss: 0.5751\n",
      "Epoch 187/200, Loss: 0.5886\n",
      "Epoch 188/200, Loss: 0.5817\n",
      "Epoch 189/200, Loss: 0.5565\n",
      "Epoch 190/200, Loss: 0.5629\n",
      "Epoch 191/200, Loss: 0.5810\n",
      "Epoch 192/200, Loss: 0.5702\n",
      "Epoch 193/200, Loss: 0.5736\n",
      "Epoch 194/200, Loss: 0.5750\n",
      "Epoch 195/200, Loss: 0.5719\n",
      "Epoch 196/200, Loss: 0.6207\n",
      "Epoch 197/200, Loss: 0.5915\n",
      "Epoch 198/200, Loss: 0.5989\n",
      "Epoch 199/200, Loss: 0.5864\n",
      "Epoch 200/200, Loss: 0.5979\n",
      "‚úÖ Ë®ìÁ∑¥Ê≠£Á¢∫ÁéáÔºö70.74%\n",
      "‚úÖ Ê∏¨Ë©¶Ê≠£Á¢∫ÁéáÔºö75.42%\n",
      "üíæ Ê®°ÂûãÂ∑≤ÂÑ≤Â≠òÁÇ∫ model_strike_predictor_debug.pth\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FF\n",
      "features\\Yu_Darvish_FF_videos_4S\\train ÂÖ±Êúâ 156 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FS\n",
      "features\\Yu_Darvish_FS_videos_4S\\train ÂÖ±Êúâ 149 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_SL\n",
      "features\\Yu_Darvish_SL_videos_4S\\train ÂÖ±Êúâ 153 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FF\n",
      "features\\Yu_Darvish_FF_videos_4S\\test ÂÖ±Êúâ 40 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FS\n",
      "features\\Yu_Darvish_FS_videos_4S\\test ÂÖ±Êúâ 38 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_SL\n",
      "features\\Yu_Darvish_SL_videos_4S\\test ÂÖ±Êúâ 40 ÂÄã .npy Ê™îÊ°à\n",
      "(458, 4000)\n",
      "üìä Ë®ìÁ∑¥Ë≥áÊñôÁ≠ÜÊï∏Ôºö458ÔºåÊ∏¨Ë©¶Ë≥áÊñôÁ≠ÜÊï∏Ôºö118\n",
      "Epoch 1/200, Loss: 2.1151\n",
      "Epoch 2/200, Loss: 54.0209\n",
      "Epoch 3/200, Loss: 25.7748\n",
      "Epoch 4/200, Loss: 6.1467\n",
      "Epoch 5/200, Loss: 7.7586\n",
      "Epoch 6/200, Loss: 4.8562\n",
      "Epoch 7/200, Loss: 6.0690\n",
      "Epoch 8/200, Loss: 4.4110\n",
      "Epoch 9/200, Loss: 5.5552\n",
      "Epoch 10/200, Loss: 4.8216\n",
      "Epoch 11/200, Loss: 3.1828\n",
      "Epoch 12/200, Loss: 4.6482\n",
      "Epoch 13/200, Loss: 3.9854\n",
      "Epoch 14/200, Loss: 2.2914\n",
      "Epoch 15/200, Loss: 2.7913\n",
      "Epoch 16/200, Loss: 3.4042\n",
      "Epoch 17/200, Loss: 2.1600\n",
      "Epoch 18/200, Loss: 2.1067\n",
      "Epoch 19/200, Loss: 2.3072\n",
      "Epoch 20/200, Loss: 1.8923\n",
      "Epoch 21/200, Loss: 1.5755\n",
      "Epoch 22/200, Loss: 1.8428\n",
      "Epoch 23/200, Loss: 1.6777\n",
      "Epoch 24/200, Loss: 1.2656\n",
      "Epoch 25/200, Loss: 1.4937\n",
      "Epoch 26/200, Loss: 1.4891\n",
      "Epoch 27/200, Loss: 1.2346\n",
      "Epoch 28/200, Loss: 1.0333\n",
      "Epoch 29/200, Loss: 1.0765\n",
      "Epoch 30/200, Loss: 1.0882\n",
      "Epoch 31/200, Loss: 0.8788\n",
      "Epoch 32/200, Loss: 0.9188\n",
      "Epoch 33/200, Loss: 0.9076\n",
      "Epoch 34/200, Loss: 0.8707\n",
      "Epoch 35/200, Loss: 0.8075\n",
      "Epoch 36/200, Loss: 0.7588\n",
      "Epoch 37/200, Loss: 0.7883\n",
      "Epoch 38/200, Loss: 0.7613\n",
      "Epoch 39/200, Loss: 0.7278\n",
      "Epoch 40/200, Loss: 0.7202\n",
      "Epoch 41/200, Loss: 0.7074\n",
      "Epoch 42/200, Loss: 0.7151\n",
      "Epoch 43/200, Loss: 0.6705\n",
      "Epoch 44/200, Loss: 0.6246\n",
      "Epoch 45/200, Loss: 0.6601\n",
      "Epoch 46/200, Loss: 0.6634\n",
      "Epoch 47/200, Loss: 0.6630\n",
      "Epoch 48/200, Loss: 0.6219\n",
      "Epoch 49/200, Loss: 0.6306\n",
      "Epoch 50/200, Loss: 0.6617\n",
      "Epoch 51/200, Loss: 0.6252\n",
      "Epoch 52/200, Loss: 0.6272\n",
      "Epoch 53/200, Loss: 0.6437\n",
      "Epoch 54/200, Loss: 0.6455\n",
      "Epoch 55/200, Loss: 0.6254\n",
      "Epoch 56/200, Loss: 0.6172\n",
      "Epoch 57/200, Loss: 0.6168\n",
      "Epoch 58/200, Loss: 0.6290\n",
      "Epoch 59/200, Loss: 0.6238\n",
      "Epoch 60/200, Loss: 0.6079\n",
      "Epoch 61/200, Loss: 0.6292\n",
      "Epoch 62/200, Loss: 0.6457\n",
      "Epoch 63/200, Loss: 0.6301\n",
      "Epoch 64/200, Loss: 0.6099\n",
      "Epoch 65/200, Loss: 0.5984\n",
      "Epoch 66/200, Loss: 0.6210\n",
      "Epoch 67/200, Loss: 0.6161\n",
      "Epoch 68/200, Loss: 0.6072\n",
      "Epoch 69/200, Loss: 0.5987\n",
      "Epoch 70/200, Loss: 0.6011\n",
      "Epoch 71/200, Loss: 0.6125\n",
      "Epoch 72/200, Loss: 0.6180\n",
      "Epoch 73/200, Loss: 0.6095\n",
      "Epoch 74/200, Loss: 0.6101\n",
      "Epoch 75/200, Loss: 0.6110\n",
      "Epoch 76/200, Loss: 0.6088\n",
      "Epoch 77/200, Loss: 0.5969\n",
      "Epoch 78/200, Loss: 0.6026\n",
      "Epoch 79/200, Loss: 0.6061\n",
      "Epoch 80/200, Loss: 0.5977\n",
      "Epoch 81/200, Loss: 0.6054\n",
      "Epoch 82/200, Loss: 0.6056\n",
      "Epoch 83/200, Loss: 0.5876\n",
      "Epoch 84/200, Loss: 0.6194\n",
      "Epoch 85/200, Loss: 0.6034\n",
      "Epoch 86/200, Loss: 0.6084\n",
      "Epoch 87/200, Loss: 0.5993\n",
      "Epoch 88/200, Loss: 0.5813\n",
      "Epoch 89/200, Loss: 0.5943\n",
      "Epoch 90/200, Loss: 0.6072\n",
      "Epoch 91/200, Loss: 0.5972\n",
      "Epoch 92/200, Loss: 0.5981\n",
      "Epoch 93/200, Loss: 0.6015\n",
      "Epoch 94/200, Loss: 0.6032\n",
      "Epoch 95/200, Loss: 0.5936\n",
      "Epoch 96/200, Loss: 0.5967\n",
      "Epoch 97/200, Loss: 0.6001\n",
      "Epoch 98/200, Loss: 0.5932\n",
      "Epoch 99/200, Loss: 0.6017\n",
      "Epoch 100/200, Loss: 0.5953\n",
      "Epoch 101/200, Loss: 0.6014\n",
      "Epoch 102/200, Loss: 0.5920\n",
      "Epoch 103/200, Loss: 0.5935\n",
      "Epoch 104/200, Loss: 0.5950\n",
      "Epoch 105/200, Loss: 0.5979\n",
      "Epoch 106/200, Loss: 0.5888\n",
      "Epoch 107/200, Loss: 0.5827\n",
      "Epoch 108/200, Loss: 0.5918\n",
      "Epoch 109/200, Loss: 0.5941\n",
      "Epoch 110/200, Loss: 0.5911\n",
      "Epoch 111/200, Loss: 0.5998\n",
      "Epoch 112/200, Loss: 0.5938\n",
      "Epoch 113/200, Loss: 0.5900\n",
      "Epoch 114/200, Loss: 0.5965\n",
      "Epoch 115/200, Loss: 0.5810\n",
      "Epoch 116/200, Loss: 0.5917\n",
      "Epoch 117/200, Loss: 0.5869\n",
      "Epoch 118/200, Loss: 0.5849\n",
      "Epoch 119/200, Loss: 0.5708\n",
      "Epoch 120/200, Loss: 0.5899\n",
      "Epoch 121/200, Loss: 0.5913\n",
      "Epoch 122/200, Loss: 0.5682\n",
      "Epoch 123/200, Loss: 0.5784\n",
      "Epoch 124/200, Loss: 0.5782\n",
      "Epoch 125/200, Loss: 0.5718\n",
      "Epoch 126/200, Loss: 0.5910\n",
      "Epoch 127/200, Loss: 0.5842\n",
      "Epoch 128/200, Loss: 0.5762\n",
      "Epoch 129/200, Loss: 0.5919\n",
      "Epoch 130/200, Loss: 0.5743\n",
      "Epoch 131/200, Loss: 0.5665\n",
      "Epoch 132/200, Loss: 0.5730\n",
      "Epoch 133/200, Loss: 0.5731\n",
      "Epoch 134/200, Loss: 0.5805\n",
      "Epoch 135/200, Loss: 0.5782\n",
      "Epoch 136/200, Loss: 0.5579\n",
      "Epoch 137/200, Loss: 0.5651\n",
      "Epoch 138/200, Loss: 0.5751\n",
      "Epoch 139/200, Loss: 0.5792\n",
      "Epoch 140/200, Loss: 0.5673\n",
      "Epoch 141/200, Loss: 0.5709\n",
      "Epoch 142/200, Loss: 0.5657\n",
      "Epoch 143/200, Loss: 0.5634\n",
      "Epoch 144/200, Loss: 0.5576\n",
      "Epoch 145/200, Loss: 0.5651\n",
      "Epoch 146/200, Loss: 0.5696\n",
      "Epoch 147/200, Loss: 0.5664\n",
      "Epoch 148/200, Loss: 0.5573\n",
      "Epoch 149/200, Loss: 0.5449\n",
      "Epoch 150/200, Loss: 0.5719\n",
      "Epoch 151/200, Loss: 0.5733\n",
      "Epoch 152/200, Loss: 0.5437\n",
      "Epoch 153/200, Loss: 0.5488\n",
      "Epoch 154/200, Loss: 0.5571\n",
      "Epoch 155/200, Loss: 0.5435\n",
      "Epoch 156/200, Loss: 0.5409\n",
      "Epoch 157/200, Loss: 0.5411\n",
      "Epoch 158/200, Loss: 0.5344\n",
      "Epoch 159/200, Loss: 0.5484\n",
      "Epoch 160/200, Loss: 0.5368\n",
      "Epoch 161/200, Loss: 0.5464\n",
      "Epoch 162/200, Loss: 0.5460\n",
      "Epoch 163/200, Loss: 0.5719\n",
      "Epoch 164/200, Loss: 0.5916\n",
      "Epoch 165/200, Loss: 0.6281\n",
      "Epoch 166/200, Loss: 0.5548\n",
      "Epoch 167/200, Loss: 0.5530\n",
      "Epoch 168/200, Loss: 0.5413\n",
      "Epoch 169/200, Loss: 0.5791\n",
      "Epoch 170/200, Loss: 0.5838\n",
      "Epoch 171/200, Loss: 0.5919\n",
      "Epoch 172/200, Loss: 0.5843\n",
      "Epoch 173/200, Loss: 0.5766\n",
      "Epoch 174/200, Loss: 0.5912\n",
      "Epoch 175/200, Loss: 0.5720\n",
      "Epoch 176/200, Loss: 0.5709\n",
      "Epoch 177/200, Loss: 0.5855\n",
      "Epoch 178/200, Loss: 0.5859\n",
      "Epoch 179/200, Loss: 0.5732\n",
      "Epoch 180/200, Loss: 0.5776\n",
      "Epoch 181/200, Loss: 0.5889\n",
      "Epoch 182/200, Loss: 0.5568\n",
      "Epoch 183/200, Loss: 0.5700\n",
      "Epoch 184/200, Loss: 0.5675\n",
      "Epoch 185/200, Loss: 0.5723\n",
      "Epoch 186/200, Loss: 0.5668\n",
      "Epoch 187/200, Loss: 0.5736\n",
      "Epoch 188/200, Loss: 0.5569\n",
      "Epoch 189/200, Loss: 0.5827\n",
      "Epoch 190/200, Loss: 0.5565\n",
      "Epoch 191/200, Loss: 0.5770\n",
      "Epoch 192/200, Loss: 0.5657\n",
      "Epoch 193/200, Loss: 0.5612\n",
      "Epoch 194/200, Loss: 0.5787\n",
      "Epoch 195/200, Loss: 0.5508\n",
      "Epoch 196/200, Loss: 0.5683\n",
      "Epoch 197/200, Loss: 0.5584\n",
      "Epoch 198/200, Loss: 0.5850\n",
      "Epoch 199/200, Loss: 0.5832\n",
      "Epoch 200/200, Loss: 0.5662\n",
      "‚úÖ Ë®ìÁ∑¥Ê≠£Á¢∫ÁéáÔºö70.74%\n",
      "‚úÖ Ê∏¨Ë©¶Ê≠£Á¢∫ÁéáÔºö75.42%\n",
      "üíæ Ê®°ÂûãÂ∑≤ÂÑ≤Â≠òÁÇ∫ model_strike_predictor_debug.pth\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FF\n",
      "features\\Yu_Darvish_FF_videos_4S\\train ÂÖ±Êúâ 156 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FS\n",
      "features\\Yu_Darvish_FS_videos_4S\\train ÂÖ±Êúâ 149 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_SL\n",
      "features\\Yu_Darvish_SL_videos_4S\\train ÂÖ±Êúâ 153 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FF\n",
      "features\\Yu_Darvish_FF_videos_4S\\test ÂÖ±Êúâ 40 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FS\n",
      "features\\Yu_Darvish_FS_videos_4S\\test ÂÖ±Êúâ 38 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_SL\n",
      "features\\Yu_Darvish_SL_videos_4S\\test ÂÖ±Êúâ 40 ÂÄã .npy Ê™îÊ°à\n",
      "(458, 4000)\n",
      "üìä Ë®ìÁ∑¥Ë≥áÊñôÁ≠ÜÊï∏Ôºö458ÔºåÊ∏¨Ë©¶Ë≥áÊñôÁ≠ÜÊï∏Ôºö118\n",
      "Epoch 1/200, Loss: 4.6965\n",
      "Epoch 2/200, Loss: 84.6652\n",
      "Epoch 3/200, Loss: 18.4094\n",
      "Epoch 4/200, Loss: 16.5419\n",
      "Epoch 5/200, Loss: 20.9452\n",
      "Epoch 6/200, Loss: 20.5385\n",
      "Epoch 7/200, Loss: 16.3961\n",
      "Epoch 8/200, Loss: 11.8308\n",
      "Epoch 9/200, Loss: 6.6384\n",
      "Epoch 10/200, Loss: 2.9125\n",
      "Epoch 11/200, Loss: 6.5729\n",
      "Epoch 12/200, Loss: 6.4635\n",
      "Epoch 13/200, Loss: 3.0710\n",
      "Epoch 14/200, Loss: 2.0308\n",
      "Epoch 15/200, Loss: 2.9391\n",
      "Epoch 16/200, Loss: 3.1993\n",
      "Epoch 17/200, Loss: 3.0605\n",
      "Epoch 18/200, Loss: 2.5052\n",
      "Epoch 19/200, Loss: 1.4028\n",
      "Epoch 20/200, Loss: 1.4080\n",
      "Epoch 21/200, Loss: 1.8776\n",
      "Epoch 22/200, Loss: 1.7662\n",
      "Epoch 23/200, Loss: 1.3654\n",
      "Epoch 24/200, Loss: 1.1464\n",
      "Epoch 25/200, Loss: 1.0963\n",
      "Epoch 26/200, Loss: 1.2243\n",
      "Epoch 27/200, Loss: 1.1254\n",
      "Epoch 28/200, Loss: 0.9947\n",
      "Epoch 29/200, Loss: 0.9511\n",
      "Epoch 30/200, Loss: 0.9452\n",
      "Epoch 31/200, Loss: 0.9058\n",
      "Epoch 32/200, Loss: 0.8284\n",
      "Epoch 33/200, Loss: 0.7665\n",
      "Epoch 34/200, Loss: 0.7905\n",
      "Epoch 35/200, Loss: 0.8231\n",
      "Epoch 36/200, Loss: 0.7717\n",
      "Epoch 37/200, Loss: 0.7479\n",
      "Epoch 38/200, Loss: 0.7384\n",
      "Epoch 39/200, Loss: 0.6970\n",
      "Epoch 40/200, Loss: 0.6504\n",
      "Epoch 41/200, Loss: 0.6630\n",
      "Epoch 42/200, Loss: 0.6511\n",
      "Epoch 43/200, Loss: 0.6358\n",
      "Epoch 44/200, Loss: 0.6427\n",
      "Epoch 45/200, Loss: 0.6440\n",
      "Epoch 46/200, Loss: 0.6302\n",
      "Epoch 47/200, Loss: 0.6612\n",
      "Epoch 48/200, Loss: 0.6526\n",
      "Epoch 49/200, Loss: 0.6435\n",
      "Epoch 50/200, Loss: 0.6272\n",
      "Epoch 51/200, Loss: 0.6244\n",
      "Epoch 52/200, Loss: 0.6105\n",
      "Epoch 53/200, Loss: 0.6161\n",
      "Epoch 54/200, Loss: 0.6174\n",
      "Epoch 55/200, Loss: 0.6148\n",
      "Epoch 56/200, Loss: 0.6191\n",
      "Epoch 57/200, Loss: 0.6061\n",
      "Epoch 58/200, Loss: 0.6017\n",
      "Epoch 59/200, Loss: 0.6132\n",
      "Epoch 60/200, Loss: 0.6121\n",
      "Epoch 61/200, Loss: 0.6185\n",
      "Epoch 62/200, Loss: 0.6187\n",
      "Epoch 63/200, Loss: 0.6067\n",
      "Epoch 64/200, Loss: 0.6042\n",
      "Epoch 65/200, Loss: 0.6178\n",
      "Epoch 66/200, Loss: 0.6021\n",
      "Epoch 67/200, Loss: 0.6011\n",
      "Epoch 68/200, Loss: 0.6040\n",
      "Epoch 69/200, Loss: 0.6058\n",
      "Epoch 70/200, Loss: 0.6050\n",
      "Epoch 71/200, Loss: 0.6011\n",
      "Epoch 72/200, Loss: 0.5958\n",
      "Epoch 73/200, Loss: 0.6110\n",
      "Epoch 74/200, Loss: 0.6083\n",
      "Epoch 75/200, Loss: 0.5991\n",
      "Epoch 76/200, Loss: 0.6028\n",
      "Epoch 77/200, Loss: 0.5968\n",
      "Epoch 78/200, Loss: 0.6061\n",
      "Epoch 79/200, Loss: 0.5963\n",
      "Epoch 80/200, Loss: 0.6006\n",
      "Epoch 81/200, Loss: 0.6002\n",
      "Epoch 82/200, Loss: 0.5993\n",
      "Epoch 83/200, Loss: 0.5966\n",
      "Epoch 84/200, Loss: 0.5975\n",
      "Epoch 85/200, Loss: 0.6043\n",
      "Epoch 86/200, Loss: 0.6022\n",
      "Epoch 87/200, Loss: 0.5965\n",
      "Epoch 88/200, Loss: 0.5999\n",
      "Epoch 89/200, Loss: 0.6022\n",
      "Epoch 90/200, Loss: 0.6020\n",
      "Epoch 91/200, Loss: 0.5993\n",
      "Epoch 92/200, Loss: 0.6004\n",
      "Epoch 93/200, Loss: 0.5999\n",
      "Epoch 94/200, Loss: 0.6034\n",
      "Epoch 95/200, Loss: 0.5996\n",
      "Epoch 96/200, Loss: 0.6044\n",
      "Epoch 97/200, Loss: 0.6003\n",
      "Epoch 98/200, Loss: 0.5927\n",
      "Epoch 99/200, Loss: 0.6025\n",
      "Epoch 100/200, Loss: 0.6006\n",
      "Epoch 101/200, Loss: 0.5909\n",
      "Epoch 102/200, Loss: 0.6043\n",
      "Epoch 103/200, Loss: 0.5964\n",
      "Epoch 104/200, Loss: 0.5934\n",
      "Epoch 105/200, Loss: 0.5993\n",
      "Epoch 106/200, Loss: 0.6129\n",
      "Epoch 107/200, Loss: 0.5945\n",
      "Epoch 108/200, Loss: 0.5967\n",
      "Epoch 109/200, Loss: 0.5925\n",
      "Epoch 110/200, Loss: 0.5972\n",
      "Epoch 111/200, Loss: 0.5943\n",
      "Epoch 112/200, Loss: 0.5949\n",
      "Epoch 113/200, Loss: 0.5916\n",
      "Epoch 114/200, Loss: 0.6007\n",
      "Epoch 115/200, Loss: 0.6020\n",
      "Epoch 116/200, Loss: 0.5864\n",
      "Epoch 117/200, Loss: 0.5930\n",
      "Epoch 118/200, Loss: 0.5950\n",
      "Epoch 119/200, Loss: 0.5878\n",
      "Epoch 120/200, Loss: 0.5918\n",
      "Epoch 121/200, Loss: 0.5950\n",
      "Epoch 122/200, Loss: 0.5849\n",
      "Epoch 123/200, Loss: 0.5883\n",
      "Epoch 124/200, Loss: 0.5883\n",
      "Epoch 125/200, Loss: 0.5946\n",
      "Epoch 126/200, Loss: 0.5890\n",
      "Epoch 127/200, Loss: 0.5975\n",
      "Epoch 128/200, Loss: 0.5911\n",
      "Epoch 129/200, Loss: 0.5885\n",
      "Epoch 130/200, Loss: 0.5916\n",
      "Epoch 131/200, Loss: 0.5864\n",
      "Epoch 132/200, Loss: 0.5814\n",
      "Epoch 133/200, Loss: 0.5865\n",
      "Epoch 134/200, Loss: 0.5851\n",
      "Epoch 135/200, Loss: 0.5874\n",
      "Epoch 136/200, Loss: 0.5949\n",
      "Epoch 137/200, Loss: 0.5918\n",
      "Epoch 138/200, Loss: 0.5823\n",
      "Epoch 139/200, Loss: 0.5744\n",
      "Epoch 140/200, Loss: 0.5810\n",
      "Epoch 141/200, Loss: 0.5832\n",
      "Epoch 142/200, Loss: 0.5919\n",
      "Epoch 143/200, Loss: 0.5792\n",
      "Epoch 144/200, Loss: 0.5743\n",
      "Epoch 145/200, Loss: 0.5689\n",
      "Epoch 146/200, Loss: 0.5805\n",
      "Epoch 147/200, Loss: 0.5808\n",
      "Epoch 148/200, Loss: 0.5844\n",
      "Epoch 149/200, Loss: 0.5779\n",
      "Epoch 150/200, Loss: 0.5792\n",
      "Epoch 151/200, Loss: 0.5825\n",
      "Epoch 152/200, Loss: 0.5829\n",
      "Epoch 153/200, Loss: 0.5745\n",
      "Epoch 154/200, Loss: 0.5791\n",
      "Epoch 155/200, Loss: 0.5707\n",
      "Epoch 156/200, Loss: 0.5776\n",
      "Epoch 157/200, Loss: 0.5800\n",
      "Epoch 158/200, Loss: 0.5764\n",
      "Epoch 159/200, Loss: 0.5727\n",
      "Epoch 160/200, Loss: 0.5717\n",
      "Epoch 161/200, Loss: 0.5841\n",
      "Epoch 162/200, Loss: 0.5716\n",
      "Epoch 163/200, Loss: 0.5782\n",
      "Epoch 164/200, Loss: 0.5749\n",
      "Epoch 165/200, Loss: 0.5762\n",
      "Epoch 166/200, Loss: 0.5755\n",
      "Epoch 167/200, Loss: 0.5747\n",
      "Epoch 168/200, Loss: 0.5797\n",
      "Epoch 169/200, Loss: 0.5738\n",
      "Epoch 170/200, Loss: 0.5700\n",
      "Epoch 171/200, Loss: 0.5661\n",
      "Epoch 172/200, Loss: 0.5600\n",
      "Epoch 173/200, Loss: 0.5735\n",
      "Epoch 174/200, Loss: 0.5703\n",
      "Epoch 175/200, Loss: 0.5752\n",
      "Epoch 176/200, Loss: 0.5617\n",
      "Epoch 177/200, Loss: 0.5754\n",
      "Epoch 178/200, Loss: 0.5643\n",
      "Epoch 179/200, Loss: 0.5668\n",
      "Epoch 180/200, Loss: 0.5594\n",
      "Epoch 181/200, Loss: 0.5729\n",
      "Epoch 182/200, Loss: 0.5580\n",
      "Epoch 183/200, Loss: 0.5653\n",
      "Epoch 184/200, Loss: 0.5753\n",
      "Epoch 185/200, Loss: 0.5684\n",
      "Epoch 186/200, Loss: 0.5658\n",
      "Epoch 187/200, Loss: 0.5675\n",
      "Epoch 188/200, Loss: 0.5574\n",
      "Epoch 189/200, Loss: 0.5548\n",
      "Epoch 190/200, Loss: 0.5604\n",
      "Epoch 191/200, Loss: 0.5678\n",
      "Epoch 192/200, Loss: 0.5725\n",
      "Epoch 193/200, Loss: 0.5645\n",
      "Epoch 194/200, Loss: 0.5591\n",
      "Epoch 195/200, Loss: 0.5704\n",
      "Epoch 196/200, Loss: 0.5567\n",
      "Epoch 197/200, Loss: 0.5834\n",
      "Epoch 198/200, Loss: 0.5476\n",
      "Epoch 199/200, Loss: 0.5703\n",
      "Epoch 200/200, Loss: 0.5711\n",
      "‚úÖ Ë®ìÁ∑¥Ê≠£Á¢∫ÁéáÔºö70.74%\n",
      "‚úÖ Ê∏¨Ë©¶Ê≠£Á¢∫ÁéáÔºö75.42%\n",
      "üíæ Ê®°ÂûãÂ∑≤ÂÑ≤Â≠òÁÇ∫ model_strike_predictor_debug.pth\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FF\n",
      "features\\Yu_Darvish_FF_videos_4S\\train ÂÖ±Êúâ 156 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FS\n",
      "features\\Yu_Darvish_FS_videos_4S\\train ÂÖ±Êúâ 149 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_SL\n",
      "features\\Yu_Darvish_SL_videos_4S\\train ÂÖ±Êúâ 153 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FF\n",
      "features\\Yu_Darvish_FF_videos_4S\\test ÂÖ±Êúâ 40 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FS\n",
      "features\\Yu_Darvish_FS_videos_4S\\test ÂÖ±Êúâ 38 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_SL\n",
      "features\\Yu_Darvish_SL_videos_4S\\test ÂÖ±Êúâ 40 ÂÄã .npy Ê™îÊ°à\n",
      "(458, 4000)\n",
      "üìä Ë®ìÁ∑¥Ë≥áÊñôÁ≠ÜÊï∏Ôºö458ÔºåÊ∏¨Ë©¶Ë≥áÊñôÁ≠ÜÊï∏Ôºö118\n",
      "Epoch 1/200, Loss: 3.1221\n",
      "Epoch 2/200, Loss: 41.9311\n",
      "Epoch 3/200, Loss: 16.1699\n",
      "Epoch 4/200, Loss: 14.3341\n",
      "Epoch 5/200, Loss: 5.7382\n",
      "Epoch 6/200, Loss: 8.2106\n",
      "Epoch 7/200, Loss: 9.5334\n",
      "Epoch 8/200, Loss: 7.3096\n",
      "Epoch 9/200, Loss: 4.1782\n",
      "Epoch 10/200, Loss: 6.5188\n",
      "Epoch 11/200, Loss: 6.2257\n",
      "Epoch 12/200, Loss: 2.7713\n",
      "Epoch 13/200, Loss: 4.0227\n",
      "Epoch 14/200, Loss: 4.7058\n",
      "Epoch 15/200, Loss: 4.0512\n",
      "Epoch 16/200, Loss: 2.6763\n",
      "Epoch 17/200, Loss: 2.4585\n",
      "Epoch 18/200, Loss: 3.2217\n",
      "Epoch 19/200, Loss: 3.1003\n",
      "Epoch 20/200, Loss: 2.0982\n",
      "Epoch 21/200, Loss: 2.4525\n",
      "Epoch 22/200, Loss: 2.6142\n",
      "Epoch 23/200, Loss: 2.4049\n",
      "Epoch 24/200, Loss: 1.7127\n",
      "Epoch 25/200, Loss: 1.5271\n",
      "Epoch 26/200, Loss: 2.0470\n",
      "Epoch 27/200, Loss: 1.9608\n",
      "Epoch 28/200, Loss: 1.2815\n",
      "Epoch 29/200, Loss: 1.4638\n",
      "Epoch 30/200, Loss: 1.6662\n",
      "Epoch 31/200, Loss: 1.5628\n",
      "Epoch 32/200, Loss: 1.3647\n",
      "Epoch 33/200, Loss: 1.1773\n",
      "Epoch 34/200, Loss: 1.4098\n",
      "Epoch 35/200, Loss: 1.4910\n",
      "Epoch 36/200, Loss: 1.0341\n",
      "Epoch 37/200, Loss: 1.1429\n",
      "Epoch 38/200, Loss: 1.1987\n",
      "Epoch 39/200, Loss: 1.1085\n",
      "Epoch 40/200, Loss: 1.0180\n",
      "Epoch 41/200, Loss: 1.0256\n",
      "Epoch 42/200, Loss: 0.8987\n",
      "Epoch 43/200, Loss: 0.9405\n",
      "Epoch 44/200, Loss: 0.8263\n",
      "Epoch 45/200, Loss: 0.7834\n",
      "Epoch 46/200, Loss: 0.8727\n",
      "Epoch 47/200, Loss: 0.8097\n",
      "Epoch 48/200, Loss: 0.8191\n",
      "Epoch 49/200, Loss: 0.8091\n",
      "Epoch 50/200, Loss: 0.7441\n",
      "Epoch 51/200, Loss: 0.7092\n",
      "Epoch 52/200, Loss: 0.7465\n",
      "Epoch 53/200, Loss: 0.7126\n",
      "Epoch 54/200, Loss: 0.7003\n",
      "Epoch 55/200, Loss: 0.7231\n",
      "Epoch 56/200, Loss: 0.7060\n",
      "Epoch 57/200, Loss: 0.6798\n",
      "Epoch 58/200, Loss: 0.6807\n",
      "Epoch 59/200, Loss: 0.6786\n",
      "Epoch 60/200, Loss: 0.6765\n",
      "Epoch 61/200, Loss: 0.6622\n",
      "Epoch 62/200, Loss: 0.6750\n",
      "Epoch 63/200, Loss: 0.6547\n",
      "Epoch 64/200, Loss: 0.6534\n",
      "Epoch 65/200, Loss: 0.6603\n",
      "Epoch 66/200, Loss: 0.6513\n",
      "Epoch 67/200, Loss: 0.6306\n",
      "Epoch 68/200, Loss: 0.6358\n",
      "Epoch 69/200, Loss: 0.6333\n",
      "Epoch 70/200, Loss: 0.6491\n",
      "Epoch 71/200, Loss: 0.6292\n",
      "Epoch 72/200, Loss: 0.6236\n",
      "Epoch 73/200, Loss: 0.6085\n",
      "Epoch 74/200, Loss: 0.6047\n",
      "Epoch 75/200, Loss: 0.6308\n",
      "Epoch 76/200, Loss: 0.6105\n",
      "Epoch 77/200, Loss: 0.6090\n",
      "Epoch 78/200, Loss: 0.6223\n",
      "Epoch 79/200, Loss: 0.6195\n",
      "Epoch 80/200, Loss: 0.6106\n",
      "Epoch 81/200, Loss: 0.6143\n",
      "Epoch 82/200, Loss: 0.6098\n",
      "Epoch 83/200, Loss: 0.6116\n",
      "Epoch 84/200, Loss: 0.6158\n",
      "Epoch 85/200, Loss: 0.6184\n",
      "Epoch 86/200, Loss: 0.5990\n",
      "Epoch 87/200, Loss: 0.6015\n",
      "Epoch 88/200, Loss: 0.5997\n",
      "Epoch 89/200, Loss: 0.6060\n",
      "Epoch 90/200, Loss: 0.5984\n",
      "Epoch 91/200, Loss: 0.5949\n",
      "Epoch 92/200, Loss: 0.6101\n",
      "Epoch 93/200, Loss: 0.6173\n",
      "Epoch 94/200, Loss: 0.5994\n",
      "Epoch 95/200, Loss: 0.5905\n",
      "Epoch 96/200, Loss: 0.6084\n",
      "Epoch 97/200, Loss: 0.6065\n",
      "Epoch 98/200, Loss: 0.6144\n",
      "Epoch 99/200, Loss: 0.5949\n",
      "Epoch 100/200, Loss: 0.5940\n",
      "Epoch 101/200, Loss: 0.5919\n",
      "Epoch 102/200, Loss: 0.5930\n",
      "Epoch 103/200, Loss: 0.5988\n",
      "Epoch 104/200, Loss: 0.6141\n",
      "Epoch 105/200, Loss: 0.6015\n",
      "Epoch 106/200, Loss: 0.5842\n",
      "Epoch 107/200, Loss: 0.5830\n",
      "Epoch 108/200, Loss: 0.5894\n",
      "Epoch 109/200, Loss: 0.5943\n",
      "Epoch 110/200, Loss: 0.5947\n",
      "Epoch 111/200, Loss: 0.5898\n",
      "Epoch 112/200, Loss: 0.5914\n",
      "Epoch 113/200, Loss: 0.5888\n",
      "Epoch 114/200, Loss: 0.5834\n",
      "Epoch 115/200, Loss: 0.5907\n",
      "Epoch 116/200, Loss: 0.5894\n",
      "Epoch 117/200, Loss: 0.5905\n",
      "Epoch 118/200, Loss: 0.5746\n",
      "Epoch 119/200, Loss: 0.5828\n",
      "Epoch 120/200, Loss: 0.5851\n",
      "Epoch 121/200, Loss: 0.5936\n",
      "Epoch 122/200, Loss: 0.5890\n",
      "Epoch 123/200, Loss: 0.5809\n",
      "Epoch 124/200, Loss: 0.5797\n",
      "Epoch 125/200, Loss: 0.5841\n",
      "Epoch 126/200, Loss: 0.5835\n",
      "Epoch 127/200, Loss: 0.5794\n",
      "Epoch 128/200, Loss: 0.5747\n",
      "Epoch 129/200, Loss: 0.5737\n",
      "Epoch 130/200, Loss: 0.5793\n",
      "Epoch 131/200, Loss: 0.5864\n",
      "Epoch 132/200, Loss: 0.5782\n",
      "Epoch 133/200, Loss: 0.5847\n",
      "Epoch 134/200, Loss: 0.5752\n",
      "Epoch 135/200, Loss: 0.5683\n",
      "Epoch 136/200, Loss: 0.5852\n",
      "Epoch 137/200, Loss: 0.5688\n",
      "Epoch 138/200, Loss: 0.5797\n",
      "Epoch 139/200, Loss: 0.5668\n",
      "Epoch 140/200, Loss: 0.5629\n",
      "Epoch 141/200, Loss: 0.5715\n",
      "Epoch 142/200, Loss: 0.5842\n",
      "Epoch 143/200, Loss: 0.5729\n",
      "Epoch 144/200, Loss: 0.5646\n",
      "Epoch 145/200, Loss: 0.5686\n",
      "Epoch 146/200, Loss: 0.5578\n",
      "Epoch 147/200, Loss: 0.5637\n",
      "Epoch 148/200, Loss: 0.5712\n",
      "Epoch 149/200, Loss: 0.5666\n",
      "Epoch 150/200, Loss: 0.5891\n",
      "Epoch 151/200, Loss: 0.5921\n",
      "Epoch 152/200, Loss: 0.5734\n",
      "Epoch 153/200, Loss: 0.5818\n",
      "Epoch 154/200, Loss: 0.5871\n",
      "Epoch 155/200, Loss: 0.5736\n",
      "Epoch 156/200, Loss: 0.5692\n",
      "Epoch 157/200, Loss: 0.5805\n",
      "Epoch 158/200, Loss: 0.5736\n",
      "Epoch 159/200, Loss: 0.5760\n",
      "Epoch 160/200, Loss: 0.5569\n",
      "Epoch 161/200, Loss: 0.5776\n",
      "Epoch 162/200, Loss: 0.5659\n",
      "Epoch 163/200, Loss: 0.5758\n",
      "Epoch 164/200, Loss: 0.5586\n",
      "Epoch 165/200, Loss: 0.5984\n",
      "Epoch 166/200, Loss: 0.5813\n",
      "Epoch 167/200, Loss: 0.5786\n",
      "Epoch 168/200, Loss: 0.5872\n",
      "Epoch 169/200, Loss: 0.5730\n",
      "Epoch 170/200, Loss: 0.5782\n",
      "Epoch 171/200, Loss: 0.5568\n",
      "Epoch 172/200, Loss: 0.5564\n",
      "Epoch 173/200, Loss: 0.5610\n",
      "Epoch 174/200, Loss: 0.5593\n",
      "Epoch 175/200, Loss: 0.5578\n",
      "Epoch 176/200, Loss: 0.5445\n",
      "Epoch 177/200, Loss: 0.5410\n",
      "Epoch 178/200, Loss: 0.5597\n",
      "Epoch 179/200, Loss: 0.5501\n",
      "Epoch 180/200, Loss: 0.5315\n",
      "Epoch 181/200, Loss: 0.5785\n",
      "Epoch 182/200, Loss: 0.5849\n",
      "Epoch 183/200, Loss: 0.5813\n",
      "Epoch 184/200, Loss: 0.5721\n",
      "Epoch 185/200, Loss: 0.5570\n",
      "Epoch 186/200, Loss: 0.5993\n",
      "Epoch 187/200, Loss: 0.6331\n",
      "Epoch 188/200, Loss: 0.6047\n",
      "Epoch 189/200, Loss: 0.5814\n",
      "Epoch 190/200, Loss: 0.5931\n",
      "Epoch 191/200, Loss: 0.5896\n",
      "Epoch 192/200, Loss: 0.6030\n",
      "Epoch 193/200, Loss: 0.5920\n",
      "Epoch 194/200, Loss: 0.5919\n",
      "Epoch 195/200, Loss: 0.5862\n",
      "Epoch 196/200, Loss: 0.5928\n",
      "Epoch 197/200, Loss: 0.5804\n",
      "Epoch 198/200, Loss: 0.5914\n",
      "Epoch 199/200, Loss: 0.5858\n",
      "Epoch 200/200, Loss: 0.5938\n",
      "‚úÖ Ë®ìÁ∑¥Ê≠£Á¢∫ÁéáÔºö70.74%\n",
      "‚úÖ Ê∏¨Ë©¶Ê≠£Á¢∫ÁéáÔºö75.42%\n",
      "üíæ Ê®°ÂûãÂ∑≤ÂÑ≤Â≠òÁÇ∫ model_strike_predictor_debug.pth\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FF\n",
      "features\\Yu_Darvish_FF_videos_4S\\train ÂÖ±Êúâ 156 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FS\n",
      "features\\Yu_Darvish_FS_videos_4S\\train ÂÖ±Êúâ 149 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_SL\n",
      "features\\Yu_Darvish_SL_videos_4S\\train ÂÖ±Êúâ 153 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FF\n",
      "features\\Yu_Darvish_FF_videos_4S\\test ÂÖ±Êúâ 40 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FS\n",
      "features\\Yu_Darvish_FS_videos_4S\\test ÂÖ±Êúâ 38 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_SL\n",
      "features\\Yu_Darvish_SL_videos_4S\\test ÂÖ±Êúâ 40 ÂÄã .npy Ê™îÊ°à\n",
      "(458, 4000)\n",
      "üìä Ë®ìÁ∑¥Ë≥áÊñôÁ≠ÜÊï∏Ôºö458ÔºåÊ∏¨Ë©¶Ë≥áÊñôÁ≠ÜÊï∏Ôºö118\n",
      "Epoch 1/200, Loss: 2.7888\n",
      "Epoch 2/200, Loss: 57.3612\n",
      "Epoch 3/200, Loss: 22.1922\n",
      "Epoch 4/200, Loss: 19.1373\n",
      "Epoch 5/200, Loss: 7.8458\n",
      "Epoch 6/200, Loss: 8.8472\n",
      "Epoch 7/200, Loss: 11.6623\n",
      "Epoch 8/200, Loss: 10.0386\n",
      "Epoch 9/200, Loss: 4.9557\n",
      "Epoch 10/200, Loss: 5.1673\n",
      "Epoch 11/200, Loss: 6.6929\n",
      "Epoch 12/200, Loss: 3.0808\n",
      "Epoch 13/200, Loss: 3.9030\n",
      "Epoch 14/200, Loss: 4.4068\n",
      "Epoch 15/200, Loss: 4.2776\n",
      "Epoch 16/200, Loss: 3.3074\n",
      "Epoch 17/200, Loss: 2.9983\n",
      "Epoch 18/200, Loss: 3.1877\n",
      "Epoch 19/200, Loss: 3.5476\n",
      "Epoch 20/200, Loss: 2.5178\n",
      "Epoch 21/200, Loss: 2.0711\n",
      "Epoch 22/200, Loss: 2.2415\n",
      "Epoch 23/200, Loss: 2.1430\n",
      "Epoch 24/200, Loss: 1.7659\n",
      "Epoch 25/200, Loss: 1.5583\n",
      "Epoch 26/200, Loss: 1.9311\n",
      "Epoch 27/200, Loss: 1.8562\n",
      "Epoch 28/200, Loss: 1.4872\n",
      "Epoch 29/200, Loss: 1.3523\n",
      "Epoch 30/200, Loss: 1.6128\n",
      "Epoch 31/200, Loss: 1.4182\n",
      "Epoch 32/200, Loss: 1.2068\n",
      "Epoch 33/200, Loss: 1.0412\n",
      "Epoch 34/200, Loss: 1.2988\n",
      "Epoch 35/200, Loss: 1.1046\n",
      "Epoch 36/200, Loss: 1.0534\n",
      "Epoch 37/200, Loss: 0.9703\n",
      "Epoch 38/200, Loss: 0.9872\n",
      "Epoch 39/200, Loss: 0.9779\n",
      "Epoch 40/200, Loss: 0.9146\n",
      "Epoch 41/200, Loss: 0.9468\n",
      "Epoch 42/200, Loss: 0.9388\n",
      "Epoch 43/200, Loss: 0.8986\n",
      "Epoch 44/200, Loss: 0.8012\n",
      "Epoch 45/200, Loss: 0.7813\n",
      "Epoch 46/200, Loss: 0.7595\n",
      "Epoch 47/200, Loss: 0.8141\n",
      "Epoch 48/200, Loss: 0.7422\n",
      "Epoch 49/200, Loss: 0.7488\n",
      "Epoch 50/200, Loss: 0.7525\n",
      "Epoch 51/200, Loss: 0.7167\n",
      "Epoch 52/200, Loss: 0.6933\n",
      "Epoch 53/200, Loss: 0.6972\n",
      "Epoch 54/200, Loss: 0.7215\n",
      "Epoch 55/200, Loss: 0.7042\n",
      "Epoch 56/200, Loss: 0.7323\n",
      "Epoch 57/200, Loss: 0.6766\n",
      "Epoch 58/200, Loss: 0.6890\n",
      "Epoch 59/200, Loss: 0.6625\n",
      "Epoch 60/200, Loss: 0.6632\n",
      "Epoch 61/200, Loss: 0.6622\n",
      "Epoch 62/200, Loss: 0.6674\n",
      "Epoch 63/200, Loss: 0.6592\n",
      "Epoch 64/200, Loss: 0.6622\n",
      "Epoch 65/200, Loss: 0.6365\n",
      "Epoch 66/200, Loss: 0.6374\n",
      "Epoch 67/200, Loss: 0.6264\n",
      "Epoch 68/200, Loss: 0.6335\n",
      "Epoch 69/200, Loss: 0.6462\n",
      "Epoch 70/200, Loss: 0.6663\n",
      "Epoch 71/200, Loss: 0.6407\n",
      "Epoch 72/200, Loss: 0.6480\n",
      "Epoch 73/200, Loss: 0.6287\n",
      "Epoch 74/200, Loss: 0.6364\n",
      "Epoch 75/200, Loss: 0.6491\n",
      "Epoch 76/200, Loss: 0.6063\n",
      "Epoch 77/200, Loss: 0.6181\n",
      "Epoch 78/200, Loss: 0.6342\n",
      "Epoch 79/200, Loss: 0.6392\n",
      "Epoch 80/200, Loss: 0.6083\n",
      "Epoch 81/200, Loss: 0.6160\n",
      "Epoch 82/200, Loss: 0.6066\n",
      "Epoch 83/200, Loss: 0.5995\n",
      "Epoch 84/200, Loss: 0.6170\n",
      "Epoch 85/200, Loss: 0.6126\n",
      "Epoch 86/200, Loss: 0.6151\n",
      "Epoch 87/200, Loss: 0.6059\n",
      "Epoch 88/200, Loss: 0.5999\n",
      "Epoch 89/200, Loss: 0.6076\n",
      "Epoch 90/200, Loss: 0.5929\n",
      "Epoch 91/200, Loss: 0.6046\n",
      "Epoch 92/200, Loss: 0.6239\n",
      "Epoch 93/200, Loss: 0.5985\n",
      "Epoch 94/200, Loss: 0.6052\n",
      "Epoch 95/200, Loss: 0.6021\n",
      "Epoch 96/200, Loss: 0.5977\n",
      "Epoch 97/200, Loss: 0.6031\n",
      "Epoch 98/200, Loss: 0.5915\n",
      "Epoch 99/200, Loss: 0.5950\n",
      "Epoch 100/200, Loss: 0.6139\n",
      "Epoch 101/200, Loss: 0.5963\n",
      "Epoch 102/200, Loss: 0.5949\n",
      "Epoch 103/200, Loss: 0.5879\n",
      "Epoch 104/200, Loss: 0.6048\n",
      "Epoch 105/200, Loss: 0.6006\n",
      "Epoch 106/200, Loss: 0.6158\n",
      "Epoch 107/200, Loss: 0.6036\n",
      "Epoch 108/200, Loss: 0.5979\n",
      "Epoch 109/200, Loss: 0.5942\n",
      "Epoch 110/200, Loss: 0.6019\n",
      "Epoch 111/200, Loss: 0.6028\n",
      "Epoch 112/200, Loss: 0.5869\n",
      "Epoch 113/200, Loss: 0.5944\n",
      "Epoch 114/200, Loss: 0.6023\n",
      "Epoch 115/200, Loss: 0.5964\n",
      "Epoch 116/200, Loss: 0.6029\n",
      "Epoch 117/200, Loss: 0.5914\n",
      "Epoch 118/200, Loss: 0.5965\n",
      "Epoch 119/200, Loss: 0.5910\n",
      "Epoch 120/200, Loss: 0.5991\n",
      "Epoch 121/200, Loss: 0.5803\n",
      "Epoch 122/200, Loss: 0.5972\n",
      "Epoch 123/200, Loss: 0.5889\n",
      "Epoch 124/200, Loss: 0.6218\n",
      "Epoch 125/200, Loss: 0.5966\n",
      "Epoch 126/200, Loss: 0.5873\n",
      "Epoch 127/200, Loss: 0.5895\n",
      "Epoch 128/200, Loss: 0.5964\n",
      "Epoch 129/200, Loss: 0.5853\n",
      "Epoch 130/200, Loss: 0.5972\n",
      "Epoch 131/200, Loss: 0.5853\n",
      "Epoch 132/200, Loss: 0.5938\n",
      "Epoch 133/200, Loss: 0.5856\n",
      "Epoch 134/200, Loss: 0.5857\n",
      "Epoch 135/200, Loss: 0.5880\n",
      "Epoch 136/200, Loss: 0.5885\n",
      "Epoch 137/200, Loss: 0.5921\n",
      "Epoch 138/200, Loss: 0.5805\n",
      "Epoch 139/200, Loss: 0.5740\n",
      "Epoch 140/200, Loss: 0.5889\n",
      "Epoch 141/200, Loss: 0.5913\n",
      "Epoch 142/200, Loss: 0.5809\n",
      "Epoch 143/200, Loss: 0.5816\n",
      "Epoch 144/200, Loss: 0.5915\n",
      "Epoch 145/200, Loss: 0.5775\n",
      "Epoch 146/200, Loss: 0.5743\n",
      "Epoch 147/200, Loss: 0.5891\n",
      "Epoch 148/200, Loss: 0.5833\n",
      "Epoch 149/200, Loss: 0.5830\n",
      "Epoch 150/200, Loss: 0.5813\n",
      "Epoch 151/200, Loss: 0.5737\n",
      "Epoch 152/200, Loss: 0.5804\n",
      "Epoch 153/200, Loss: 0.5717\n",
      "Epoch 154/200, Loss: 0.5768\n",
      "Epoch 155/200, Loss: 0.5767\n",
      "Epoch 156/200, Loss: 0.5750\n",
      "Epoch 157/200, Loss: 0.5793\n",
      "Epoch 158/200, Loss: 0.5773\n",
      "Epoch 159/200, Loss: 0.5652\n",
      "Epoch 160/200, Loss: 0.5660\n",
      "Epoch 161/200, Loss: 0.5865\n",
      "Epoch 162/200, Loss: 0.5957\n",
      "Epoch 163/200, Loss: 0.5686\n",
      "Epoch 164/200, Loss: 0.5754\n",
      "Epoch 165/200, Loss: 0.5833\n",
      "Epoch 166/200, Loss: 0.5792\n",
      "Epoch 167/200, Loss: 0.5776\n",
      "Epoch 168/200, Loss: 0.5783\n",
      "Epoch 169/200, Loss: 0.5755\n",
      "Epoch 170/200, Loss: 0.5845\n",
      "Epoch 171/200, Loss: 0.5794\n",
      "Epoch 172/200, Loss: 0.5755\n",
      "Epoch 173/200, Loss: 0.5726\n",
      "Epoch 174/200, Loss: 0.5765\n",
      "Epoch 175/200, Loss: 0.5637\n",
      "Epoch 176/200, Loss: 0.5658\n",
      "Epoch 177/200, Loss: 0.5655\n",
      "Epoch 178/200, Loss: 0.5695\n",
      "Epoch 179/200, Loss: 0.5726\n",
      "Epoch 180/200, Loss: 0.5682\n",
      "Epoch 181/200, Loss: 0.5652\n",
      "Epoch 182/200, Loss: 0.5742\n",
      "Epoch 183/200, Loss: 0.5670\n",
      "Epoch 184/200, Loss: 0.5704\n",
      "Epoch 185/200, Loss: 0.5613\n",
      "Epoch 186/200, Loss: 0.5518\n",
      "Epoch 187/200, Loss: 0.5526\n",
      "Epoch 188/200, Loss: 0.5620\n",
      "Epoch 189/200, Loss: 0.5493\n",
      "Epoch 190/200, Loss: 0.5581\n",
      "Epoch 191/200, Loss: 0.5565\n",
      "Epoch 192/200, Loss: 0.5515\n",
      "Epoch 193/200, Loss: 0.5456\n",
      "Epoch 194/200, Loss: 0.5752\n",
      "Epoch 195/200, Loss: 0.5525\n",
      "Epoch 196/200, Loss: 0.5551\n",
      "Epoch 197/200, Loss: 0.5545\n",
      "Epoch 198/200, Loss: 0.5508\n",
      "Epoch 199/200, Loss: 0.5433\n",
      "Epoch 200/200, Loss: 0.5579\n",
      "‚úÖ Ë®ìÁ∑¥Ê≠£Á¢∫ÁéáÔºö74.24%\n",
      "‚úÖ Ê∏¨Ë©¶Ê≠£Á¢∫ÁéáÔºö74.58%\n",
      "üíæ Ê®°ÂûãÂ∑≤ÂÑ≤Â≠òÁÇ∫ model_strike_predictor_debug.pth\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FF\n",
      "features\\Yu_Darvish_FF_videos_4S\\train ÂÖ±Êúâ 156 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FS\n",
      "features\\Yu_Darvish_FS_videos_4S\\train ÂÖ±Êúâ 149 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_SL\n",
      "features\\Yu_Darvish_SL_videos_4S\\train ÂÖ±Êúâ 153 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FF\n",
      "features\\Yu_Darvish_FF_videos_4S\\test ÂÖ±Êúâ 40 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_FS\n",
      "features\\Yu_Darvish_FS_videos_4S\\test ÂÖ±Êúâ 38 ÂÄã .npy Ê™îÊ°à\n",
      "üìÇ ËôïÁêÜÈÅ∏ÊâãÔºöYu_Darvish_SL\n",
      "features\\Yu_Darvish_SL_videos_4S\\test ÂÖ±Êúâ 40 ÂÄã .npy Ê™îÊ°à\n",
      "(458, 4000)\n",
      "üìä Ë®ìÁ∑¥Ë≥áÊñôÁ≠ÜÊï∏Ôºö458ÔºåÊ∏¨Ë©¶Ë≥áÊñôÁ≠ÜÊï∏Ôºö118\n",
      "Epoch 1/200, Loss: 1.6308\n",
      "Epoch 2/200, Loss: 36.3735\n",
      "Epoch 3/200, Loss: 14.2799\n",
      "Epoch 4/200, Loss: 14.9435\n",
      "Epoch 5/200, Loss: 7.6545\n",
      "Epoch 6/200, Loss: 6.0221\n",
      "Epoch 7/200, Loss: 5.2310\n",
      "Epoch 8/200, Loss: 3.6721\n",
      "Epoch 9/200, Loss: 4.3514\n",
      "Epoch 10/200, Loss: 3.8807\n",
      "Epoch 11/200, Loss: 3.0232\n",
      "Epoch 12/200, Loss: 4.5558\n",
      "Epoch 13/200, Loss: 3.6420\n",
      "Epoch 14/200, Loss: 3.3323\n",
      "Epoch 15/200, Loss: 2.9007\n",
      "Epoch 16/200, Loss: 3.5169\n",
      "Epoch 17/200, Loss: 2.6445\n",
      "Epoch 18/200, Loss: 2.4580\n",
      "Epoch 19/200, Loss: 2.8264\n",
      "Epoch 20/200, Loss: 2.0410\n",
      "Epoch 21/200, Loss: 2.2622\n",
      "Epoch 22/200, Loss: 2.2022\n",
      "Epoch 23/200, Loss: 2.1641\n",
      "Epoch 24/200, Loss: 1.5704\n",
      "Epoch 25/200, Loss: 1.9082\n",
      "Epoch 26/200, Loss: 1.8202\n",
      "Epoch 27/200, Loss: 1.4896\n",
      "Epoch 28/200, Loss: 1.5330\n",
      "Epoch 29/200, Loss: 1.6194\n",
      "Epoch 30/200, Loss: 1.4902\n",
      "Epoch 31/200, Loss: 1.1955\n",
      "Epoch 32/200, Loss: 1.3478\n",
      "Epoch 33/200, Loss: 1.2606\n",
      "Epoch 34/200, Loss: 1.0797\n",
      "Epoch 35/200, Loss: 1.0967\n",
      "Epoch 36/200, Loss: 1.0152\n",
      "Epoch 37/200, Loss: 1.0766\n",
      "Epoch 38/200, Loss: 1.0827\n",
      "Epoch 39/200, Loss: 1.0624\n",
      "Epoch 40/200, Loss: 1.0110\n",
      "Epoch 41/200, Loss: 0.9782\n",
      "Epoch 42/200, Loss: 0.9091\n",
      "Epoch 43/200, Loss: 0.8571\n",
      "Epoch 44/200, Loss: 0.8342\n",
      "Epoch 45/200, Loss: 0.7717\n",
      "Epoch 46/200, Loss: 0.8048\n",
      "Epoch 47/200, Loss: 0.7445\n",
      "Epoch 48/200, Loss: 0.8009\n",
      "Epoch 49/200, Loss: 0.7245\n",
      "Epoch 50/200, Loss: 0.7656\n",
      "Epoch 51/200, Loss: 0.7447\n",
      "Epoch 52/200, Loss: 0.6971\n",
      "Epoch 53/200, Loss: 0.7475\n",
      "Epoch 54/200, Loss: 0.6796\n",
      "Epoch 55/200, Loss: 0.6975\n",
      "Epoch 56/200, Loss: 0.6825\n",
      "Epoch 57/200, Loss: 0.7136\n",
      "Epoch 58/200, Loss: 0.7110\n",
      "Epoch 59/200, Loss: 0.6567\n",
      "Epoch 60/200, Loss: 0.6978\n",
      "Epoch 61/200, Loss: 0.6828\n",
      "Epoch 62/200, Loss: 0.6519\n",
      "Epoch 63/200, Loss: 0.6771\n",
      "Epoch 64/200, Loss: 0.6704\n",
      "Epoch 65/200, Loss: 0.6394\n",
      "Epoch 66/200, Loss: 0.6606\n",
      "Epoch 67/200, Loss: 0.6509\n",
      "Epoch 68/200, Loss: 0.6419\n",
      "Epoch 69/200, Loss: 0.6320\n",
      "Epoch 70/200, Loss: 0.6339\n",
      "Epoch 71/200, Loss: 0.6319\n",
      "Epoch 72/200, Loss: 0.6656\n",
      "Epoch 73/200, Loss: 0.6190\n",
      "Epoch 74/200, Loss: 0.6374\n",
      "Epoch 75/200, Loss: 0.6403\n",
      "Epoch 76/200, Loss: 0.6200\n",
      "Epoch 77/200, Loss: 0.6303\n",
      "Epoch 78/200, Loss: 0.6068\n",
      "Epoch 79/200, Loss: 0.6023\n",
      "Epoch 80/200, Loss: 0.6022\n",
      "Epoch 81/200, Loss: 0.6486\n",
      "Epoch 82/200, Loss: 0.6037\n",
      "Epoch 83/200, Loss: 0.6148\n",
      "Epoch 84/200, Loss: 0.6116\n",
      "Epoch 85/200, Loss: 0.6049\n",
      "Epoch 86/200, Loss: 0.6085\n",
      "Epoch 87/200, Loss: 0.6028\n",
      "Epoch 88/200, Loss: 0.6100\n",
      "Epoch 89/200, Loss: 0.6101\n",
      "Epoch 90/200, Loss: 0.6107\n",
      "Epoch 91/200, Loss: 0.6114\n",
      "Epoch 92/200, Loss: 0.6064\n",
      "Epoch 93/200, Loss: 0.6045\n",
      "Epoch 94/200, Loss: 0.5892\n",
      "Epoch 95/200, Loss: 0.5929\n",
      "Epoch 96/200, Loss: 0.6234\n",
      "Epoch 97/200, Loss: 0.5857\n",
      "Epoch 98/200, Loss: 0.6035\n",
      "Epoch 99/200, Loss: 0.6048\n",
      "Epoch 100/200, Loss: 0.6236\n",
      "Epoch 101/200, Loss: 0.5868\n",
      "Epoch 102/200, Loss: 0.6050\n",
      "Epoch 103/200, Loss: 0.5966\n",
      "Epoch 104/200, Loss: 0.5953\n",
      "Epoch 105/200, Loss: 0.5931\n",
      "Epoch 106/200, Loss: 0.5908\n",
      "Epoch 107/200, Loss: 0.6034\n",
      "Epoch 108/200, Loss: 0.5816\n",
      "Epoch 109/200, Loss: 0.6022\n",
      "Epoch 110/200, Loss: 0.5975\n",
      "Epoch 111/200, Loss: 0.5918\n",
      "Epoch 112/200, Loss: 0.5969\n",
      "Epoch 113/200, Loss: 0.5780\n",
      "Epoch 114/200, Loss: 0.5928\n",
      "Epoch 115/200, Loss: 0.5962\n",
      "Epoch 116/200, Loss: 0.5988\n",
      "Epoch 117/200, Loss: 0.5730\n",
      "Epoch 118/200, Loss: 0.5845\n",
      "Epoch 119/200, Loss: 0.5791\n",
      "Epoch 120/200, Loss: 0.5839\n",
      "Epoch 121/200, Loss: 0.5920\n",
      "Epoch 122/200, Loss: 0.5910\n",
      "Epoch 123/200, Loss: 0.5872\n",
      "Epoch 124/200, Loss: 0.5851\n",
      "Epoch 125/200, Loss: 0.5877\n",
      "Epoch 126/200, Loss: 0.5833\n",
      "Epoch 127/200, Loss: 0.5850\n",
      "Epoch 128/200, Loss: 0.5866\n",
      "Epoch 129/200, Loss: 0.5652\n",
      "Epoch 130/200, Loss: 0.5883\n",
      "Epoch 131/200, Loss: 0.5821\n",
      "Epoch 132/200, Loss: 0.5728\n",
      "Epoch 133/200, Loss: 0.5728\n",
      "Epoch 134/200, Loss: 0.5789\n",
      "Epoch 135/200, Loss: 0.5854\n",
      "Epoch 136/200, Loss: 0.5912\n",
      "Epoch 137/200, Loss: 0.5694\n",
      "Epoch 138/200, Loss: 0.5716\n",
      "Epoch 139/200, Loss: 0.5777\n",
      "Epoch 140/200, Loss: 0.5824\n",
      "Epoch 141/200, Loss: 0.5730\n",
      "Epoch 142/200, Loss: 0.5760\n",
      "Epoch 143/200, Loss: 0.5725\n",
      "Epoch 144/200, Loss: 0.5719\n",
      "Epoch 145/200, Loss: 0.5786\n",
      "Epoch 146/200, Loss: 0.5533\n",
      "Epoch 147/200, Loss: 0.5792\n",
      "Epoch 148/200, Loss: 0.5663\n",
      "Epoch 149/200, Loss: 0.5698\n",
      "Epoch 150/200, Loss: 0.5652\n",
      "Epoch 151/200, Loss: 0.5557\n",
      "Epoch 152/200, Loss: 0.5568\n",
      "Epoch 153/200, Loss: 0.5667\n",
      "Epoch 154/200, Loss: 0.5697\n",
      "Epoch 155/200, Loss: 0.5659\n",
      "Epoch 156/200, Loss: 0.5754\n",
      "Epoch 157/200, Loss: 0.5464\n",
      "Epoch 158/200, Loss: 0.5666\n",
      "Epoch 159/200, Loss: 0.5528\n",
      "Epoch 160/200, Loss: 0.5765\n",
      "Epoch 161/200, Loss: 0.5443\n",
      "Epoch 162/200, Loss: 0.5763\n",
      "Epoch 163/200, Loss: 0.5604\n",
      "Epoch 164/200, Loss: 0.5494\n",
      "Epoch 165/200, Loss: 0.5560\n",
      "Epoch 166/200, Loss: 0.5369\n",
      "Epoch 167/200, Loss: 0.5716\n",
      "Epoch 168/200, Loss: 0.5543\n",
      "Epoch 169/200, Loss: 0.5536\n",
      "Epoch 170/200, Loss: 0.5547\n",
      "Epoch 171/200, Loss: 0.5666\n",
      "Epoch 172/200, Loss: 0.5460\n",
      "Epoch 173/200, Loss: 0.5571\n",
      "Epoch 174/200, Loss: 0.5610\n",
      "Epoch 175/200, Loss: 0.5394\n",
      "Epoch 176/200, Loss: 0.5491\n",
      "Epoch 177/200, Loss: 0.5561\n",
      "Epoch 178/200, Loss: 0.5692\n",
      "Epoch 179/200, Loss: 0.5526\n",
      "Epoch 180/200, Loss: 0.5340\n",
      "Epoch 181/200, Loss: 0.5455\n",
      "Epoch 182/200, Loss: 0.5509\n",
      "Epoch 183/200, Loss: 0.5827\n",
      "Epoch 184/200, Loss: 0.5669\n",
      "Epoch 185/200, Loss: 0.5768\n",
      "Epoch 186/200, Loss: 0.5666\n",
      "Epoch 187/200, Loss: 0.5681\n",
      "Epoch 188/200, Loss: 0.5669\n",
      "Epoch 189/200, Loss: 0.5700\n",
      "Epoch 190/200, Loss: 0.5714\n",
      "Epoch 191/200, Loss: 0.5612\n",
      "Epoch 192/200, Loss: 0.5426\n",
      "Epoch 193/200, Loss: 0.5456\n",
      "Epoch 194/200, Loss: 0.5810\n",
      "Epoch 195/200, Loss: 0.5558\n",
      "Epoch 196/200, Loss: 0.5817\n",
      "Epoch 197/200, Loss: 0.5819\n",
      "Epoch 198/200, Loss: 0.5832\n",
      "Epoch 199/200, Loss: 0.5764\n",
      "Epoch 200/200, Loss: 0.5529\n",
      "‚úÖ Ë®ìÁ∑¥Ê≠£Á¢∫ÁéáÔºö71.40%\n",
      "‚úÖ Ê∏¨Ë©¶Ê≠£Á¢∫ÁéáÔºö74.58%\n",
      "üíæ Ê®°ÂûãÂ∑≤ÂÑ≤Â≠òÁÇ∫ model_strike_predictor_debug.pth\n",
      "[0.7542372881355932, 0.7372881355932204, 0.7542372881355932, 0.6694915254237288, 0.7542372881355932, 0.7542372881355932, 0.7542372881355932, 0.7542372881355932, 0.7457627118644068, 0.7457627118644068]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ÈÄôÂÄãÂáΩÊï∏Âè™È†àÁµ¶ÁâπÂæµË≥áÊñôÂ§æ\n",
    "# ÊúÉÂú®‰∏ªË≥áÊñôÂ§æÂ∫ï‰∏ãÂ∞ãÊâæÂêÑÈÅ∏ÊâãË≥áÊñôÂ∫ï‰∏ãÁöÑtrain or test Ë≥áÊñôÂ§æ\n",
    "# ÈÄ≤ÂÖ• train or test Ë≥áÊñôÂ§æÂæåÊúÉÂ∞á npyÊ™îÊ°àÈÉΩÊãøÂà∞ ‰ª•ÂèäÂà©Áî®player_csv.loc[file_number - 1, 'description']\n",
    "# ÂéªÊâæÂ∞çÊáâlabel\n",
    "def load_features_and_labels(features_dir):\n",
    "    def collect_data(split):\n",
    "        X, y = [], []\n",
    "        for subdir in os.listdir(features_dir):\n",
    "            player_name = subdir.split('_video')[0]\n",
    "            print(f\"üìÇ ËôïÁêÜÈÅ∏ÊâãÔºö{player_name}\")\n",
    "            try:\n",
    "                player_csv_path = f'data/{player_name}_videos_4S/{player_name}.csv'\n",
    "                player_csv = pd.read_csv(player_csv_path)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå ËÆÄÂèñ CSV Â§±ÊïóÔºö{player_csv_path}ÔºåÈåØË™§Ôºö{e}\")\n",
    "                continue\n",
    "\n",
    "            split_path = os.path.join(features_dir, subdir, split)\n",
    "            if not os.path.isdir(split_path):\n",
    "                continue\n",
    "            \n",
    "            npy_files = [f for f in os.listdir(split_path) if f.endswith('.npy')]\n",
    "            print(f\"{split_path} ÂÖ±Êúâ {len(npy_files)} ÂÄã .npy Ê™îÊ°à\")\n",
    "            for file in npy_files:\n",
    "                if file.endswith(\".npy\"):\n",
    "                    npy_path = os.path.join(split_path, file)\n",
    "                    try:\n",
    "                        file_number = int(file.replace('pitch_', '').replace('.npy', ''))\n",
    "                        features = np.load(npy_path)#.reshape(4,10,100)\n",
    "                        features = features.flatten()\n",
    "                        \n",
    "                        # ÂèñÂæóÂ∞çÊáâÁöÑÊ®ôÁ±§Ôºà‰æãÂ¶Ç zoneÔºâ\n",
    "                        #label = player_csv.loc[file_number - 1, 'zone']\n",
    "                        #label = 1 if str(label) in [str(i) for i in range(1, 10)] else 0\n",
    "                        \n",
    "                        # description\n",
    "                        label = player_csv.loc[file_number-1, 'description']\n",
    "                        label = 1 if \"strike\" in str(label).lower() else 0\n",
    "\n",
    "                        X.append(features)\n",
    "                        y.append(label)\n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ùå Â§±ÊïóËÆÄÂèñ {file}Ôºö{e}\")\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    X_train, y_train = collect_data(\"train\")\n",
    "    X_test, y_test = collect_data(\"test\")\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# prepare_and_train_v3.py\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_all_random_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class SimpleTCN(nn.Module):\n",
    "    def __init__(self, input_size=400 * 12, num_classes=2):\n",
    "        super(SimpleTCN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "'''\n",
    "class SimpleTCN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(SimpleTCN, self).__init__()\n",
    "        # Ëº∏ÂÖ•: (batch, 4, 10, 100)\n",
    "        # ÂÖàÂêà‰Ωµ frames Âíå features Á∂≠Â∫¶\n",
    "        self.conv1 = nn.Conv1d(in_channels=40, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, 4, 10, 100)\n",
    "        x = x.view(x.size(0), -1, x.size(-1))  # (batch, 4*10, 100)\n",
    "        x = self.conv1(x)                     # (batch, 64, 100)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)                     # (batch, 128, 100)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool(x)                      # (batch, 128, 1)\n",
    "        x = x.squeeze(-1)                     # (batch, 128)\n",
    "        x = self.fc(x)                        # (batch, num_classes)\n",
    "        return x\n",
    "'''\n",
    "\n",
    "def train_and_evaluate():\n",
    "    X_train, y_train, X_test, y_test = load_features_and_labels(\"features\")\n",
    "    print(X_train.shape)\n",
    "\n",
    "    # Êñ∞ÁöÑÂÅöÊ≥ï\n",
    "    y_all = np.concatenate([y_train, y_test])\n",
    "    le = LabelEncoder()\n",
    "    y_all_encoded = le.fit_transform(y_all)\n",
    "    y_train = y_all_encoded[:len(y_train)]\n",
    "    y_test = y_all_encoded[len(y_train):]\n",
    "    n_class = len(np.unique(y_all_encoded))\n",
    "\n",
    "    print(f\"üìä Ë®ìÁ∑¥Ë≥áÊñôÁ≠ÜÊï∏Ôºö{len(X_train)}ÔºåÊ∏¨Ë©¶Ë≥áÊñôÁ≠ÜÊï∏Ôºö{len(X_test)}\")\n",
    "\n",
    "    model = SimpleTCN(input_size=X_train.shape[1],num_classes=n_class)\n",
    "    #model = SimpleTCN()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "    model.train()\n",
    "    epochs = 200\n",
    "    for epoch in range(epochs):\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Ë®ìÁ∑¥Ê≠£Á¢∫Áéá\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_pred = model(X_train_tensor).argmax(dim=1).numpy()\n",
    "        acc = accuracy_score(y_train, y_pred)\n",
    "        print(f\"‚úÖ Ë®ìÁ∑¥Ê≠£Á¢∫ÁéáÔºö{acc:.2%}\")\n",
    "\n",
    "    # Ê∏¨Ë©¶\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "        y_pred = model(X_test_tensor).argmax(dim=1).numpy()\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        print(f\"‚úÖ Ê∏¨Ë©¶Ê≠£Á¢∫ÁéáÔºö{acc:.2%}\")\n",
    "\n",
    "    # ÂÑ≤Â≠òÊ®°Âûã\n",
    "    torch.save(model.state_dict(), \"model_strike_predictor_debug.pth\")\n",
    "    print(\"üíæ Ê®°ÂûãÂ∑≤ÂÑ≤Â≠òÁÇ∫ model_strike_predictor_debug.pth\")\n",
    "    return acc\n",
    "\n",
    "\n",
    "test_acc_list = []\n",
    "for i in range(10):\n",
    "    set_all_random_seeds(i)\n",
    "    test_acc_list.append(train_and_evaluate())\n",
    "print(test_acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "bf9a6c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7542372881355932,\n",
       " 0.7372881355932204,\n",
       " 0.7542372881355932,\n",
       " 0.6694915254237288,\n",
       " 0.7542372881355932,\n",
       " 0.7542372881355932,\n",
       " 0.7542372881355932,\n",
       " 0.7542372881355932,\n",
       " 0.7457627118644068,\n",
       " 0.7457627118644068]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99598125",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.7542372881355932,\n",
    " 0.7372881355932204,\n",
    " 0.7542372881355932,\n",
    " 0.6694915254237288,\n",
    " 0.7542372881355932,\n",
    " 0.7542372881355932,\n",
    " 0.7542372881355932,\n",
    " 0.7542372881355932,\n",
    " 0.7457627118644068,\n",
    " 0.7457627118644068]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "041722ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7542372881355932,\n",
       " 0.6694915254237288,\n",
       " 0.6610169491525424,\n",
       " 0.7033898305084746,\n",
       " 0.6610169491525424,\n",
       " 0.6949152542372882,\n",
       " 0.6610169491525424,\n",
       " 0.6864406779661016,\n",
       " 0.6949152542372882,\n",
       " 0.6864406779661016]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0.7542372881355932,\n",
    " 0.6694915254237288,\n",
    " 0.6610169491525424,\n",
    " 0.7033898305084746,\n",
    " 0.6610169491525424,\n",
    " 0.6949152542372882,\n",
    " 0.6610169491525424,\n",
    " 0.6864406779661016,\n",
    " 0.6949152542372882,\n",
    " 0.6864406779661016]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pose39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
